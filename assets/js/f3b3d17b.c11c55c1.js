"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[281],{4763:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-2-digital-twin/week-5-high-fidelity-rendering-in-unity/chapter-9-unity-rendering-pipelines","title":"Unity Rendering Pipelines","description":"Overview","source":"@site/docs/module-2-digital-twin/week-5-high-fidelity-rendering-in-unity/chapter-9-unity-rendering-pipelines.md","sourceDirName":"module-2-digital-twin/week-5-high-fidelity-rendering-in-unity","slug":"/module-2-digital-twin/week-5-high-fidelity-rendering-in-unity/chapter-9-unity-rendering-pipelines","permalink":"/hackathon_textbook_ai_robotics/docs/module-2-digital-twin/week-5-high-fidelity-rendering-in-unity/chapter-9-unity-rendering-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadwaheedairi/hackathon_textbook_ai_robotics/edit/main/my-website/docs/module-2-digital-twin/week-5-high-fidelity-rendering-in-unity/chapter-9-unity-rendering-pipelines.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"Unity Rendering Pipelines","sidebar_label":"Chapter 9: Unity Rendering","sidebar_position":9},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 8: Gazebo ROS 2 Integration","permalink":"/hackathon_textbook_ai_robotics/docs/module-2-digital-twin/week-4-physics-simulation-in-gazebo/chapter-8-gazebo-ros2-integration"},"next":{"title":"Chapter 10: Sensor Simulation","permalink":"/hackathon_textbook_ai_robotics/docs/module-2-digital-twin/week-5-high-fidelity-rendering-in-unity/chapter-10-sensor-simulation-unity"}}');var r=n(4848),l=n(8453);const a={title:"Unity Rendering Pipelines",sidebar_label:"Chapter 9: Unity Rendering",sidebar_position:9},s="Chapter 9: Unity Rendering Pipelines",o={},d=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Unity Rendering Pipelines",id:"unity-rendering-pipelines",level:2},{value:"Built-in Render Pipeline",id:"built-in-render-pipeline",level:3},{value:"Universal Render Pipeline (URP)",id:"universal-render-pipeline-urp",level:3},{value:"High Definition Render Pipeline (HDRP)",id:"high-definition-render-pipeline-hdrp",level:3},{value:"Physically-Based Rendering (PBR)",id:"physically-based-rendering-pbr",level:2},{value:"Albedo (Base Color)",id:"albedo-base-color",level:3},{value:"Metallic",id:"metallic",level:3},{value:"Smoothness/Roughness",id:"smoothnessroughness",level:3},{value:"Normal Maps",id:"normal-maps",level:3},{value:"Occlusion Maps",id:"occlusion-maps",level:3},{value:"Lighting Systems in Unity",id:"lighting-systems-in-unity",level:2},{value:"Real-time Lighting",id:"real-time-lighting",level:3},{value:"Baked Lighting",id:"baked-lighting",level:3},{value:"Mixed Lighting",id:"mixed-lighting",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Unity C# Script for Camera Sensor Simulation",id:"unity-c-script-for-camera-sensor-simulation",level:3},{value:"Unity C# Script for Material Configuration",id:"unity-c-script-for-material-configuration",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"What&#39;s Next",id:"whats-next",level:2}];function c(e){const i={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"chapter-9-unity-rendering-pipelines",children:"Chapter 9: Unity Rendering Pipelines"})}),"\n",(0,r.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(i.p,{children:"This chapter introduces Unity's rendering pipelines for creating photorealistic robot simulation environments. You'll learn about physically-based rendering, lighting systems, and how to configure Unity for high-fidelity visual simulation."}),"\n",(0,r.jsx)(i.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsxs)(i.admonition,{title:"Learning Objectives",type:"info",children:[(0,r.jsx)(i.p,{children:"By the end of this chapter, you will be able to:"}),(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Understand Unity's rendering pipeline options (URP, HDRP)"}),"\n",(0,r.jsx)(i.li,{children:"Implement physically-based rendering (PBR) materials"}),"\n",(0,r.jsx)(i.li,{children:"Configure advanced lighting systems for realistic environments"}),"\n",(0,r.jsx)(i.li,{children:"Optimize rendering performance for real-time applications"}),"\n"]})]}),"\n",(0,r.jsx)(i.h2,{id:"unity-rendering-pipelines",children:"Unity Rendering Pipelines"}),"\n",(0,r.jsx)(i.p,{children:"Unity offers three main rendering pipelines optimized for different use cases: Built-in Render Pipeline, Universal Render Pipeline (URP), and High Definition Render Pipeline (HDRP)."}),"\n",(0,r.jsx)(i.h3,{id:"built-in-render-pipeline",children:"Built-in Render Pipeline"}),"\n",(0,r.jsx)(i.p,{children:"The legacy rendering pipeline that offers basic rendering capabilities and is suitable for simple applications. While still functional, it lacks many of the advanced features available in the newer pipelines."}),"\n",(0,r.jsx)(i.h3,{id:"universal-render-pipeline-urp",children:"Universal Render Pipeline (URP)"}),"\n",(0,r.jsx)(i.p,{children:"A lightweight, flexible rendering pipeline designed for performance across a wide range of platforms. URP provides a good balance between visual quality and performance, making it suitable for mobile robotics applications and applications requiring high frame rates."}),"\n",(0,r.jsx)(i.p,{children:"Key features of URP:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Lightweight and efficient"}),"\n",(0,r.jsx)(i.li,{children:"Supports 2D and 3D rendering"}),"\n",(0,r.jsx)(i.li,{children:"Customizable render passes"}),"\n",(0,r.jsx)(i.li,{children:"Built-in post-processing effects"}),"\n",(0,r.jsx)(i.li,{children:"Shader Graph integration"}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"high-definition-render-pipeline-hdrp",children:"High Definition Render Pipeline (HDRP)"}),"\n",(0,r.jsx)(i.p,{children:"A state-of-the-art rendering pipeline designed for high-fidelity visuals on powerful hardware. HDRP is ideal for applications requiring photorealistic rendering, such as digital twin creation and high-quality sensor simulation."}),"\n",(0,r.jsx)(i.p,{children:"Key features of HDRP:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Physically-based rendering"}),"\n",(0,r.jsx)(i.li,{children:"Real-time ray tracing"}),"\n",(0,r.jsx)(i.li,{children:"Advanced lighting models"}),"\n",(0,r.jsx)(i.li,{children:"Global illumination (Baked and realtime)"}),"\n",(0,r.jsx)(i.li,{children:"Volumetric effects"}),"\n",(0,r.jsx)(i.li,{children:"Advanced post-processing stack"}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"physically-based-rendering-pbr",children:"Physically-Based Rendering (PBR)"}),"\n",(0,r.jsx)(i.p,{children:"Physically-Based Rendering is a methodology that simulates light-object interactions using real-world physics principles. PBR materials in Unity are defined by several key properties."}),"\n",(0,r.jsx)(i.h3,{id:"albedo-base-color",children:"Albedo (Base Color)"}),"\n",(0,r.jsx)(i.p,{children:"The base color of the material without lighting considerations. This represents the color of the surface when illuminated by white light."}),"\n",(0,r.jsx)(i.h3,{id:"metallic",children:"Metallic"}),"\n",(0,r.jsx)(i.p,{children:"Controls whether a surface behaves like a metal or a non-metal. Metallic surfaces reflect light as colored reflections, while non-metallic surfaces reflect light as white highlights."}),"\n",(0,r.jsx)(i.h3,{id:"smoothnessroughness",children:"Smoothness/Roughness"}),"\n",(0,r.jsx)(i.p,{children:"Controls the microsurface detail of the material, affecting how light scatters. Smooth surfaces create sharp reflections, while rough surfaces create diffuse reflections."}),"\n",(0,r.jsx)(i.h3,{id:"normal-maps",children:"Normal Maps"}),"\n",(0,r.jsx)(i.p,{children:"Provide detailed surface geometry information without increasing polygon count. Normal maps create the illusion of complex surface details like scratches, bumps, and grooves."}),"\n",(0,r.jsx)(i.h3,{id:"occlusion-maps",children:"Occlusion Maps"}),"\n",(0,r.jsx)(i.p,{children:"Simulate the shadowing effect of small surface details, enhancing the perception of depth and contact between surfaces."}),"\n",(0,r.jsx)(i.h2,{id:"lighting-systems-in-unity",children:"Lighting Systems in Unity"}),"\n",(0,r.jsx)(i.p,{children:"Unity provides several lighting systems that can be used to create realistic illumination."}),"\n",(0,r.jsx)(i.h3,{id:"real-time-lighting",children:"Real-time Lighting"}),"\n",(0,r.jsx)(i.p,{children:"Lights that affect objects dynamically during gameplay. Real-time lighting provides interactive illumination but can be computationally expensive."}),"\n",(0,r.jsx)(i.p,{children:"Types of real-time lights:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Directional lights: Simulate distant light sources like the sun"}),"\n",(0,r.jsx)(i.li,{children:"Point lights: Emit light in all directions from a point"}),"\n",(0,r.jsx)(i.li,{children:"Spot lights: Emit light in a cone shape"}),"\n",(0,r.jsx)(i.li,{children:"Area lights: Emit light from a surface area (baked only in some pipelines)"}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"baked-lighting",children:"Baked Lighting"}),"\n",(0,r.jsx)(i.p,{children:"Lighting that is precomputed and stored in lightmaps. Baked lighting provides high-quality global illumination but cannot change at runtime."}),"\n",(0,r.jsx)(i.h3,{id:"mixed-lighting",children:"Mixed Lighting"}),"\n",(0,r.jsx)(i.p,{children:"Combines real-time and baked lighting, allowing for static lighting to be baked while enabling dynamic shadows from moving objects."}),"\n",(0,r.jsx)(i.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,r.jsx)(i.h3,{id:"unity-c-script-for-camera-sensor-simulation",children:"Unity C# Script for Camera Sensor Simulation"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-csharp",children:'using System;\nusing UnityEngine;\nusing System.Collections;\nusing System.IO;\n\n[RequireComponent(typeof(Camera))]\npublic class CameraSensor : MonoBehaviour\n{\n    [Header("Camera Settings")]\n    public int imageWidth = 640;\n    public int imageHeight = 480;\n    public float fieldOfView = 60f;\n    public string outputDirectory = "CameraOutput";\n\n    [Header("Sensor Simulation")]\n    public bool simulateDepth = false;\n    public float minDepth = 0.1f;\n    public float maxDepth = 100f;\n    public bool simulateSemanticSegmentation = false;\n\n    private Camera cam;\n    private RenderTexture renderTexture;\n    private Texture2D outputTexture;\n\n    void Start()\n    {\n        cam = GetComponent<Camera>();\n        SetupCamera();\n        CreateRenderTexture();\n    }\n\n    void SetupCamera()\n    {\n        cam.fieldOfView = fieldOfView;\n        cam.enabled = false;\n    }\n\n    void CreateRenderTexture()\n    {\n        renderTexture = new RenderTexture(imageWidth, imageHeight, 24);\n        renderTexture.format = RenderTextureFormat.ARGB32;\n        renderTexture.antiAliasing = 1;\n        renderTexture.Create();\n\n        outputTexture = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);\n    }\n\n    public void CaptureImage()\n    {\n        cam.targetTexture = renderTexture;\n        cam.Render();\n\n        RenderTexture.active = renderTexture;\n        outputTexture.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n        outputTexture.Apply();\n\n        RenderTexture.active = null;\n        cam.targetTexture = null;\n\n        SaveImage(outputTexture, "rgb_image");\n    }\n\n    public void CaptureDepthImage()\n    {\n        if (!simulateDepth) return;\n\n        RenderTexture depthRT = RenderTexture.GetTemporary(\n            imageWidth, imageHeight, 24, RenderTextureFormat.RFloat);\n\n        cam.SetTargetBuffers(depthRT.colorBuffer, depthRT.depthBuffer);\n        cam.RenderWithShader(Shader.Find("Hidden/DepthOnly"), "");\n\n        RenderTexture.active = depthRT;\n        Texture2D depthTexture = new Texture2D(imageWidth, imageHeight, TextureFormat.RFloat, false);\n        depthTexture.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);\n        depthTexture.Apply();\n\n        RenderTexture.active = null;\n        cam.ResetReplacementShader();\n\n        SaveImage(depthTexture, "depth_image");\n\n        RenderTexture.ReleaseTemporary(depthRT);\n        DestroyImmediate(depthTexture);\n    }\n\n    private void SaveImage(Texture2D texture, string prefix)\n    {\n        byte[] bytes = texture.EncodeToPNG();\n\n        string fullDir = Path.Combine(Application.dataPath, outputDirectory);\n        if (!Directory.Exists(fullDir))\n        {\n            Directory.CreateDirectory(fullDir);\n        }\n\n        string filename = $"{prefix}_{DateTime.Now:yyyyMMdd_HHmmss}.png";\n        string filepath = Path.Combine(fullDir, filename);\n\n        File.WriteAllBytes(filepath, bytes);\n        Debug.Log($"Image saved to: {filepath}");\n    }\n\n    void OnDestroy()\n    {\n        if (renderTexture != null)\n        {\n            renderTexture.Release();\n        }\n        if (outputTexture != null)\n        {\n            DestroyImmediate(outputTexture);\n        }\n    }\n}\n'})}),"\n",(0,r.jsx)(i.h3,{id:"unity-c-script-for-material-configuration",children:"Unity C# Script for Material Configuration"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class PBRMaterialController : MonoBehaviour\n{\n    [Header("PBR Properties")]\n    [Range(0f, 1f)]\n    public float metallic = 0f;\n\n    [Range(0f, 1f)]\n    public float smoothness = 0.5f;\n\n    public Color albedoColor = Color.white;\n\n    public Texture2D albedoTexture;\n    public Texture2D normalMap;\n    public Texture2D metallicMap;\n    public Texture2D occlusionMap;\n\n    private Material material;\n    private Renderer objectRenderer;\n\n    void Start()\n    {\n        objectRenderer = GetComponent<Renderer>();\n        if (objectRenderer != null)\n        {\n            material = objectRenderer.material;\n            UpdateMaterial();\n        }\n    }\n\n    void Update()\n    {\n        if (material != null)\n        {\n            UpdateMaterial();\n        }\n    }\n\n    void UpdateMaterial()\n    {\n        material.SetFloat("_Metallic", metallic);\n        material.SetFloat("_Glossiness", smoothness);\n        material.SetColor("_Color", albedoColor);\n\n        if (albedoTexture != null)\n            material.SetTexture("_MainTex", albedoTexture);\n\n        if (normalMap != null)\n            material.SetTexture("_BumpMap", normalMap);\n\n        if (metallicMap != null)\n            material.SetTexture("_MetallicGlossMap", metallicMap);\n\n        if (occlusionMap != null)\n            material.SetTexture("_OcclusionMap", occlusionMap);\n    }\n\n    public void SetMetallic(float value)\n    {\n        metallic = Mathf.Clamp01(value);\n        UpdateMaterial();\n    }\n\n    public void SetSmoothness(float value)\n    {\n        smoothness = Mathf.Clamp01(value);\n        UpdateMaterial();\n    }\n\n    public void SetAlbedoColor(Color color)\n    {\n        albedoColor = color;\n        UpdateMaterial();\n    }\n}\n'})}),"\n",(0,r.jsx)(i.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(i.p,{children:"Unity's rendering pipelines provide powerful tools for creating photorealistic robot simulation environments. Understanding PBR materials and lighting systems enables the creation of high-fidelity digital twins that closely match real-world conditions for effective sim-to-real transfer."}),"\n",(0,r.jsx)(i.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsx)(i.admonition,{title:"Key Takeaways",type:"tip",children:(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"HDRP provides photorealistic rendering for high-fidelity simulations"}),"\n",(0,r.jsx)(i.li,{children:"PBR materials simulate realistic light-surface interactions"}),"\n",(0,r.jsx)(i.li,{children:"Multiple lighting modes enable flexible environment creation"}),"\n",(0,r.jsx)(i.li,{children:"Camera sensors can capture RGB, depth, and segmentation data"}),"\n"]})}),"\n",(0,r.jsx)(i.h2,{id:"whats-next",children:"What's Next"}),"\n",(0,r.jsx)(i.p,{children:"In the next chapter, we'll explore sensor simulation in Unity, learning how to implement LIDAR, IMU, and other sensors for comprehensive robot perception testing."})]})}function h(e={}){const{wrapper:i}={...(0,l.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>s});var t=n(6540);const r={},l=t.createContext(r);function a(e){const i=t.useContext(l);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function s(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(l.Provider,{value:i},e.children)}}}]);