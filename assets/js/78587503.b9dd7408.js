"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9190],{3857:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vision-language-action/week-11-system-integration/chapter-22-vision-multimodal","title":"Vision and Multimodal Systems","description":"Overview","source":"@site/docs/module-4-vision-language-action/week-11-system-integration/chapter-22-vision-multimodal.md","sourceDirName":"module-4-vision-language-action/week-11-system-integration","slug":"/module-4-vision-language-action/week-11-system-integration/chapter-22-vision-multimodal","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-11-system-integration/chapter-22-vision-multimodal","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadwaheedairi/hackathon_textbook_ai_robotics/edit/main/my-website/docs/module-4-vision-language-action/week-11-system-integration/chapter-22-vision-multimodal.md","tags":[],"version":"current","sidebarPosition":22,"frontMatter":{"title":"Vision and Multimodal Systems","sidebar_label":"Chapter 22: Vision & Multimodal","sidebar_position":22},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 21: System Architecture","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-11-system-integration/chapter-21-system-architecture"},"next":{"title":"Chapter 23: Real-World Deployment","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-12-advanced-deployment/chapter-23-real-world-deployment"}}');var o=n(4848),s=n(8453);const a={title:"Vision and Multimodal Systems",sidebar_label:"Chapter 22: Vision & Multimodal",sidebar_position:22},r="Chapter 22: Vision and Multimodal Systems",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Vision Processing for Humanoid Systems",id:"vision-processing-for-humanoid-systems",level:2},{value:"Multimodal Integration",id:"multimodal-integration",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Vision Processor",id:"vision-processor",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"What&#39;s Next",id:"whats-next",level:2}];function d(e){const i={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(i.header,{children:(0,o.jsx)(i.h1,{id:"chapter-22-vision-and-multimodal-systems",children:"Chapter 22: Vision and Multimodal Systems"})}),"\n",(0,o.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(i.p,{children:"This chapter explores integrating vision systems with other sensor modalities for comprehensive robot perception. You'll learn how to combine visual, audio, and tactile information for robust environmental understanding."}),"\n",(0,o.jsx)(i.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(i.admonition,{title:"Learning Objectives",type:"info",children:[(0,o.jsx)(i.p,{children:"By the end of this chapter, you will be able to:"}),(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Integrate computer vision with robot perception systems"}),"\n",(0,o.jsx)(i.li,{children:"Implement multimodal sensor fusion techniques"}),"\n",(0,o.jsx)(i.li,{children:"Combine vision, voice, and gesture recognition"}),"\n",(0,o.jsx)(i.li,{children:"Create context-aware robot behaviors"}),"\n"]})]}),"\n",(0,o.jsx)(i.h2,{id:"vision-processing-for-humanoid-systems",children:"Vision Processing for Humanoid Systems"}),"\n",(0,o.jsx)(i.p,{children:"Computer vision enables robots to perceive and understand their environment through visual data, supporting object detection, pose estimation, scene understanding, and visual navigation."}),"\n",(0,o.jsx)(i.h2,{id:"multimodal-integration",children:"Multimodal Integration"}),"\n",(0,o.jsx)(i.p,{children:"Combining multiple input modalities including voice input (speech recognition and command parsing), vision input (object detection and scene understanding), gesture input (hand tracking and gesture recognition), and context management (environmental awareness and state tracking)."}),"\n",(0,o.jsx)(i.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,o.jsx)(i.h3,{id:"vision-processor",children:"Vision Processor"}),"\n",(0,o.jsx)(i.pre,{children:(0,o.jsx)(i.code,{className:"language-python",children:"import cv2\nimport numpy as np\nimport torch\nfrom ultralytics import YOLO\nfrom geometry_msgs.msg import Point\n\nclass VisionProcessor:\n    def __init__(self):\n        self.object_detector = YOLO('yolov8n.pt')\n        self.pose_estimator = YOLO('yolov8n-pose.pt')\n        self.camera_matrix = None\n        self.dist_coeffs = None\n\n    def process_frame(self, image_msg):\n        cv_image = self._ros_to_cv2(image_msg)\n        objects = self._detect_objects(cv_image)\n        poses = self._estimate_poses(cv_image)\n        object_positions = self._calculate_3d_positions(objects)\n\n        return {\n            'objects': object_positions,\n            'poses': poses,\n            'scene_description': self._describe_scene(objects, poses)\n        }\n"})}),"\n",(0,o.jsx)(i.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(i.p,{children:"Multimodal integration combines vision, voice, and other sensors for comprehensive robot perception. By fusing information from multiple modalities, robots achieve robust environmental understanding and can handle complex interaction scenarios."}),"\n",(0,o.jsx)(i.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsx)(i.admonition,{title:"Key Takeaways",type:"tip",children:(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Vision systems provide essential spatial and object information"}),"\n",(0,o.jsx)(i.li,{children:"Multimodal fusion improves perception robustness"}),"\n",(0,o.jsx)(i.li,{children:"Context-aware processing enables adaptive robot behaviors"}),"\n",(0,o.jsx)(i.li,{children:"Proper sensor calibration ensures accurate multimodal integration"}),"\n"]})}),"\n",(0,o.jsx)(i.h2,{id:"whats-next",children:"What's Next"}),"\n",(0,o.jsx)(i.p,{children:"In the next chapter, we'll explore real-world deployment strategies, learning how to transition from simulation to physical robot systems with proper calibration and testing."})]})}function m(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>r});var t=n(6540);const o={},s=t.createContext(o);function a(e){const i=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:i},e.children)}}}]);