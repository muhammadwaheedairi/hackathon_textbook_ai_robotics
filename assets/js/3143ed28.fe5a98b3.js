"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[685],{813:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper","title":"Voice-to-Action with OpenAI Whisper","description":"Introduction","source":"@site/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadwaheedairi/hackathon_textbook_ai_robotics/edit/main/my-website/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"Voice-to-Action with OpenAI Whisper","sidebar_label":"Week 9 - Voice-to-Action with OpenAI Whisper","sidebar_position":9},"sidebar":"textbookSidebar","previous":{"title":"Week 8 - Isaac Sim Reinforcement Learning","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/week-8-isaac-sim-reinforcement-learning"},"next":{"title":"Week 10 - Cognitive Planning with LLMs","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-10-cognitive-planning"}}');var s=i(4848),o=i(8453);const r={title:"Voice-to-Action with OpenAI Whisper",sidebar_label:"Week 9 - Voice-to-Action with OpenAI Whisper",sidebar_position:9},a="Week 9: Voice-to-Action with OpenAI Whisper",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"1. Introduction to Voice-to-Action Systems",id:"1-introduction-to-voice-to-action-systems",level:2},{value:"1.1 What are Voice-to-Action Systems?",id:"11-what-are-voice-to-action-systems",level:3},{value:"1.2 Applications in Robotics",id:"12-applications-in-robotics",level:3},{value:"1.3 System Architecture",id:"13-system-architecture",level:3},{value:"2. OpenAI Whisper for Speech Recognition",id:"2-openai-whisper-for-speech-recognition",level:2},{value:"2.1 What is OpenAI Whisper?",id:"21-what-is-openai-whisper",level:3},{value:"2.2 Whisper Model Variants",id:"22-whisper-model-variants",level:3},{value:"2.3 Installation and Setup",id:"23-installation-and-setup",level:3},{value:"2.4 Basic Whisper Usage",id:"24-basic-whisper-usage",level:3},{value:"3. Real-Time Voice Recognition",id:"3-real-time-voice-recognition",level:2},{value:"3.1 Audio Input Processing",id:"31-audio-input-processing",level:3},{value:"3.2 Audio Stream Processing",id:"32-audio-stream-processing",level:3},{value:"3.3 Voice Activity Detection",id:"33-voice-activity-detection",level:3},{value:"4. Natural Language Understanding",id:"4-natural-language-understanding",level:2},{value:"4.1 Command Parsing",id:"41-command-parsing",level:3},{value:"4.2 Intent Recognition",id:"42-intent-recognition",level:3},{value:"4.3 Context Awareness",id:"43-context-awareness",level:3},{value:"5. Integration with ROS 2",id:"5-integration-with-ros-2",level:2},{value:"5.1 ROS 2 Node Structure",id:"51-ros-2-node-structure",level:3},{value:"5.2 Action Mapping",id:"52-action-mapping",level:3},{value:"6. Voice Command Vocabulary",id:"6-voice-command-vocabulary",level:2},{value:"6.1 Basic Navigation Commands",id:"61-basic-navigation-commands",level:3},{value:"6.2 Manipulation Commands",id:"62-manipulation-commands",level:3},{value:"6.3 System Commands",id:"63-system-commands",level:3},{value:"7. Performance Optimization",id:"7-performance-optimization",level:2},{value:"7.1 Model Optimization",id:"71-model-optimization",level:3},{value:"7.2 Real-time Processing",id:"72-real-time-processing",level:3},{value:"7.3 Accuracy Improvements",id:"73-accuracy-improvements",level:3},{value:"8. Advanced Features",id:"8-advanced-features",level:2},{value:"8.1 Multi-language Support",id:"81-multi-language-support",level:3},{value:"8.2 Custom Training",id:"82-custom-training",level:3},{value:"8.3 Voice Authentication",id:"83-voice-authentication",level:3},{value:"9. Practical Implementation",id:"9-practical-implementation",level:2},{value:"9.1 Complete Voice-to-Action System",id:"91-complete-voice-to-action-system",level:3},{value:"10. Testing and Validation",id:"10-testing-and-validation",level:2},{value:"10.1 Unit Testing",id:"101-unit-testing",level:3},{value:"10.2 Integration Testing",id:"102-integration-testing",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"week-9-voice-to-action-with-openai-whisper",children:"Week 9: Voice-to-Action with OpenAI Whisper"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Welcome to Week 9 of the Vision-Language-Action (VLA) module! This week we'll explore how to implement voice-to-action systems using OpenAI Whisper for speech recognition. We'll learn how to convert spoken commands into actionable robot behaviors, creating intuitive human-robot interaction interfaces. This technology enables robots to understand natural language commands and execute corresponding actions in real-world environments."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this week, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the fundamentals of speech recognition and voice-to-action systems"}),"\n",(0,s.jsx)(n.li,{children:"Install and configure OpenAI Whisper for real-time speech recognition"}),"\n",(0,s.jsx)(n.li,{children:"Process audio input and convert speech to text commands"}),"\n",(0,s.jsx)(n.li,{children:"Map recognized commands to robot actions"}),"\n",(0,s.jsx)(n.li,{children:"Integrate voice commands with ROS 2 control systems"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before starting this week's content, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of ROS 2 fundamentals (Weeks 1-3)"}),"\n",(0,s.jsx)(n.li,{children:"Basic knowledge of audio processing concepts"}),"\n",(0,s.jsx)(n.li,{children:"Experience with Python programming"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with natural language processing concepts"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"1-introduction-to-voice-to-action-systems",children:"1. Introduction to Voice-to-Action Systems"}),"\n",(0,s.jsx)(n.h3,{id:"11-what-are-voice-to-action-systems",children:"1.1 What are Voice-to-Action Systems?"}),"\n",(0,s.jsx)(n.p,{children:"Voice-to-action systems enable robots to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Recognize spoken commands using speech recognition"}),"\n",(0,s.jsx)(n.li,{children:"Interpret natural language instructions"}),"\n",(0,s.jsx)(n.li,{children:"Convert voice commands into executable robot actions"}),"\n",(0,s.jsx)(n.li,{children:"Provide intuitive human-robot interaction"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"12-applications-in-robotics",children:"1.2 Applications in Robotics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Assistive robotics for elderly care"}),"\n",(0,s.jsx)(n.li,{children:"Industrial automation with voice commands"}),"\n",(0,s.jsx)(n.li,{children:"Educational robotics"}),"\n",(0,s.jsx)(n.li,{children:"Service robotics in homes and offices"}),"\n",(0,s.jsx)(n.li,{children:"Search and rescue operations"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"13-system-architecture",children:"1.3 System Architecture"}),"\n",(0,s.jsx)(n.p,{children:"A typical voice-to-action system includes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Input"}),": Microphones for capturing speech"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech Recognition"}),": Converting audio to text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Processing"}),": Understanding command intent"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Mapping"}),": Converting commands to robot actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Execution"}),": Robot control and feedback"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"2-openai-whisper-for-speech-recognition",children:"2. OpenAI Whisper for Speech Recognition"}),"\n",(0,s.jsx)(n.h3,{id:"21-what-is-openai-whisper",children:"2.1 What is OpenAI Whisper?"}),"\n",(0,s.jsx)(n.p,{children:"OpenAI Whisper is a state-of-the-art speech recognition model that:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Provides high accuracy across multiple languages"}),"\n",(0,s.jsx)(n.li,{children:"Handles various accents and speaking styles"}),"\n",(0,s.jsx)(n.li,{children:"Works well in noisy environments"}),"\n",(0,s.jsx)(n.li,{children:"Supports real-time and batch processing"}),"\n",(0,s.jsx)(n.li,{children:"Is available as an open-source model"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"22-whisper-model-variants",children:"2.2 Whisper Model Variants"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tiny"}),": Fastest, least accurate (76MB)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"base"}),": Good balance of speed and accuracy (145MB)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"small"}),": Better accuracy, moderate speed (484MB)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"medium"}),": High accuracy, slower (1.5GB)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"large"}),": Highest accuracy, slowest (3.0GB)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"23-installation-and-setup",children:"2.3 Installation and Setup"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n# For GPU acceleration\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"})}),"\n",(0,s.jsx)(n.h3,{id:"24-basic-whisper-usage",children:"2.4 Basic Whisper Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import whisper\n\n# Load model (downloads if not present)\nmodel = whisper.load_model("small")\n\n# Transcribe audio file\nresult = model.transcribe("command.wav")\nprint(result["text"])\n'})}),"\n",(0,s.jsx)(n.h2,{id:"3-real-time-voice-recognition",children:"3. Real-Time Voice Recognition"}),"\n",(0,s.jsx)(n.h3,{id:"31-audio-input-processing",children:"3.1 Audio Input Processing"}),"\n",(0,s.jsx)(n.p,{children:"For real-time voice recognition, we need to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Capture audio from microphone"}),"\n",(0,s.jsx)(n.li,{children:"Process audio in chunks"}),"\n",(0,s.jsx)(n.li,{children:"Handle streaming input efficiently"}),"\n",(0,s.jsx)(n.li,{children:"Filter out background noise"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"32-audio-stream-processing",children:"3.2 Audio Stream Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport numpy as np\nimport queue\nimport threading\nimport whisper\nimport torch\n\nclass VoiceToAction:\n    def __init__(self, model_size="small"):\n        # Initialize Whisper model\n        self.model = whisper.load_model(model_size)\n        self.audio_queue = queue.Queue()\n\n        # Audio parameters\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 16000\n        self.chunk = 1024\n\n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n\n    def start_listening(self):\n        # Open audio stream\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        # Start audio recording thread\n        threading.Thread(target=self.record_audio, args=(stream,), daemon=True).start()\n\n    def record_audio(self, stream):\n        while True:\n            data = stream.read(self.chunk)\n            self.audio_queue.put(data)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"33-voice-activity-detection",children:"3.3 Voice Activity Detection"}),"\n",(0,s.jsx)(n.p,{children:"Implement voice activity detection to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Detect when speech starts and ends"}),"\n",(0,s.jsx)(n.li,{children:"Reduce unnecessary processing"}),"\n",(0,s.jsx)(n.li,{children:"Improve real-time performance"}),"\n",(0,s.jsx)(n.li,{children:"Handle background noise"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"4-natural-language-understanding",children:"4. Natural Language Understanding"}),"\n",(0,s.jsx)(n.h3,{id:"41-command-parsing",children:"4.1 Command Parsing"}),"\n",(0,s.jsx)(n.p,{children:"Convert recognized text into structured commands:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Extract action verbs (move, pick, place, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Identify objects and locations"}),"\n",(0,s.jsx)(n.li,{children:"Parse numerical parameters"}),"\n",(0,s.jsx)(n.li,{children:"Handle complex multi-step commands"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"42-intent-recognition",children:"4.2 Intent Recognition"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import re\n\nclass CommandParser:\n    def __init__(self):\n        # Define command patterns\n        self.move_patterns = [\n            r'move to (.+)',\n            r'go to (.+)',\n            r'go to the (.+)',\n            r'navigate to (.+)'\n        ]\n\n        self.pick_patterns = [\n            r'pick up the (.+)',\n            r'pick the (.+)',\n            r'grab the (.+)',\n            r'take the (.+)'\n        ]\n\n        self.place_patterns = [\n            r'place it on the (.+)',\n            r'put it on the (.+)',\n            r'place the (.+) on the (.+)',\n            r'put the (.+) on the (.+)'\n        ]\n\n    def parse_command(self, text):\n        text = text.lower().strip()\n\n        # Check move patterns\n        for pattern in self.move_patterns:\n            match = re.search(pattern, text)\n            if match:\n                return {'action': 'move', 'target': match.group(1)}\n\n        # Check pick patterns\n        for pattern in self.pick_patterns:\n            match = re.search(pattern, text)\n            if match:\n                return {'action': 'pick', 'object': match.group(1)}\n\n        # Check place patterns\n        for pattern in self.place_patterns:\n            match = re.search(pattern, text)\n            if match:\n                if len(match.groups()) == 1:\n                    return {'action': 'place', 'target': match.group(1)}\n                else:\n                    return {'action': 'place', 'object': match.group(1), 'target': match.group(2)}\n\n        return {'action': 'unknown', 'raw': text}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"43-context-awareness",children:"4.3 Context Awareness"}),"\n",(0,s.jsx)(n.p,{children:"Implement context awareness for:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding relative positions"}),"\n",(0,s.jsx)(n.li,{children:"Handling ambiguous commands"}),"\n",(0,s.jsx)(n.li,{children:"Maintaining conversation state"}),"\n",(0,s.jsx)(n.li,{children:"Learning user preferences"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"5-integration-with-ros-2",children:"5. Integration with ROS 2"}),"\n",(0,s.jsx)(n.h3,{id:"51-ros-2-node-structure",children:"5.1 ROS 2 Node Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom sensor_msgs.msg import AudioData\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__('voice_command_node')\n\n        # Publishers for robot commands\n        self.move_pub = self.create_publisher(Pose, 'move_command', 10)\n        self.action_pub = self.create_publisher(String, 'action_command', 10)\n\n        # Subscriber for audio input\n        self.audio_sub = self.create_subscription(\n            AudioData, 'audio_input', self.audio_callback, 10)\n\n        # Timer for processing audio queue\n        self.timer = self.create_timer(0.1, self.process_audio)\n\n        # Initialize Whisper model\n        self.whisper_model = whisper.load_model(\"small\")\n\n    def audio_callback(self, msg):\n        # Process audio data and convert to text\n        audio_array = np.frombuffer(msg.data, dtype=np.int16)\n        result = self.whisper_model.transcribe(audio_array)\n        command_text = result[\"text\"]\n\n        # Parse and execute command\n        self.execute_command(command_text)\n\n    def execute_command(self, command_text):\n        parser = CommandParser()\n        parsed_command = parser.parse_command(command_text)\n\n        if parsed_command['action'] == 'move':\n            self.send_move_command(parsed_command['target'])\n        elif parsed_command['action'] == 'pick':\n            self.send_pick_command(parsed_command['object'])\n        elif parsed_command['action'] == 'place':\n            self.send_place_command(parsed_command['target'])\n"})}),"\n",(0,s.jsx)(n.h3,{id:"52-action-mapping",children:"5.2 Action Mapping"}),"\n",(0,s.jsx)(n.p,{children:"Map recognized commands to ROS 2 actions:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Navigation commands \u2192 Navigation2 stack"}),"\n",(0,s.jsx)(n.li,{children:"Manipulation commands \u2192 MoveIt! or custom controllers"}),"\n",(0,s.jsx)(n.li,{children:"System commands \u2192 Service calls"}),"\n",(0,s.jsx)(n.li,{children:"Query commands \u2192 Parameter requests"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"6-voice-command-vocabulary",children:"6. Voice Command Vocabulary"}),"\n",(0,s.jsx)(n.h3,{id:"61-basic-navigation-commands",children:"6.1 Basic Navigation Commands"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Go to the kitchen"'}),"\n",(0,s.jsx)(n.li,{children:'"Move to the table"'}),"\n",(0,s.jsx)(n.li,{children:'"Navigate to the charging station"'}),"\n",(0,s.jsx)(n.li,{children:'"Return to base"'}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"62-manipulation-commands",children:"6.2 Manipulation Commands"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Pick up the red cup"'}),"\n",(0,s.jsx)(n.li,{children:'"Place the book on the shelf"'}),"\n",(0,s.jsx)(n.li,{children:'"Open the door"'}),"\n",(0,s.jsx)(n.li,{children:'"Close the drawer"'}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"63-system-commands",children:"6.3 System Commands"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Stop" or "Halt"'}),"\n",(0,s.jsx)(n.li,{children:'"Pause"'}),"\n",(0,s.jsx)(n.li,{children:'"Resume"'}),"\n",(0,s.jsx)(n.li,{children:'"Shutdown"'}),"\n",(0,s.jsx)(n.li,{children:'"Status"'}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"7-performance-optimization",children:"7. Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"71-model-optimization",children:"7.1 Model Optimization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use appropriate model size for your hardware"}),"\n",(0,s.jsx)(n.li,{children:"Implement model quantization"}),"\n",(0,s.jsx)(n.li,{children:"Use GPU acceleration when available"}),"\n",(0,s.jsx)(n.li,{children:"Cache frequently used models"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"72-real-time-processing",children:"7.2 Real-time Processing"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Optimize audio buffer sizes"}),"\n",(0,s.jsx)(n.li,{children:"Use efficient threading"}),"\n",(0,s.jsx)(n.li,{children:"Implement command queuing"}),"\n",(0,s.jsx)(n.li,{children:"Handle processing delays gracefully"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"73-accuracy-improvements",children:"7.3 Accuracy Improvements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Train custom language models"}),"\n",(0,s.jsx)(n.li,{children:"Implement command confirmation"}),"\n",(0,s.jsx)(n.li,{children:"Use context-aware recognition"}),"\n",(0,s.jsx)(n.li,{children:"Add error correction mechanisms"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"8-advanced-features",children:"8. Advanced Features"}),"\n",(0,s.jsx)(n.h3,{id:"81-multi-language-support",children:"8.1 Multi-language Support"}),"\n",(0,s.jsx)(n.p,{children:"Whisper supports multiple languages:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"English, German, French, Spanish, Italian"}),"\n",(0,s.jsx)(n.li,{children:"Portuguese, Polish, Chinese, Japanese, Korean"}),"\n",(0,s.jsx)(n.li,{children:"And many more languages"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"82-custom-training",children:"8.2 Custom Training"}),"\n",(0,s.jsx)(n.p,{children:"Fine-tune Whisper for:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Domain-specific vocabulary"}),"\n",(0,s.jsx)(n.li,{children:"Specific accents or dialects"}),"\n",(0,s.jsx)(n.li,{children:"Noisy environments"}),"\n",(0,s.jsx)(n.li,{children:"Specialized command sets"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"83-voice-authentication",children:"8.3 Voice Authentication"}),"\n",(0,s.jsx)(n.p,{children:"Implement voice biometrics for:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"User identification"}),"\n",(0,s.jsx)(n.li,{children:"Security verification"}),"\n",(0,s.jsx)(n.li,{children:"Personalized responses"}),"\n",(0,s.jsx)(n.li,{children:"Access control"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"9-practical-implementation",children:"9. Practical Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"91-complete-voice-to-action-system",children:"9.1 Complete Voice-to-Action System"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nimport whisper\nimport pyaudio\nimport numpy as np\nimport threading\nimport queue\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\n\nclass CompleteVoiceToActionNode(Node):\n    def __init__(self):\n        super().__init__('complete_voice_to_action')\n\n        # Initialize Whisper model\n        self.model = whisper.load_model(\"small\")\n\n        # Audio processing setup\n        self.audio_queue = queue.Queue()\n        self.setup_audio()\n\n        # ROS 2 publishers\n        self.command_pub = self.create_publisher(String, 'robot_commands', 10)\n\n        # Start audio processing thread\n        self.processing_thread = threading.Thread(\n            target=self.process_audio_stream, daemon=True)\n        self.processing_thread.start()\n\n    def setup_audio(self):\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=16000,\n            input=True,\n            frames_per_buffer=1024\n        )\n\n    def process_audio_stream(self):\n        while rclpy.ok():\n            # Read audio chunk\n            data = self.stream.read(1024)\n            audio_array = np.frombuffer(data, dtype=np.int16)\n\n            # Process with Whisper\n            result = self.model.transcribe(audio_array)\n            text = result[\"text\"]\n\n            if text.strip():  # If we have recognized text\n                self.process_command(text)\n\n    def process_command(self, text):\n        # Publish command to robot\n        msg = String()\n        msg.data = text\n        self.command_pub.publish(msg)\n\n        self.get_logger().info(f'Recognized: {text}')\n"})}),"\n",(0,s.jsx)(n.h2,{id:"10-testing-and-validation",children:"10. Testing and Validation"}),"\n",(0,s.jsx)(n.h3,{id:"101-unit-testing",children:"10.1 Unit Testing"}),"\n",(0,s.jsx)(n.p,{children:"Test individual components:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Audio input processing"}),"\n",(0,s.jsx)(n.li,{children:"Speech recognition accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Command parsing"}),"\n",(0,s.jsx)(n.li,{children:"ROS 2 integration"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"102-integration-testing",children:"10.2 Integration Testing"}),"\n",(0,s.jsx)(n.p,{children:"Test the complete system:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"End-to-end voice command processing"}),"\n",(0,s.jsx)(n.li,{children:"Robot response accuracy"}),"\n",(0,s.jsx)(n.li,{children:"System robustness"}),"\n",(0,s.jsx)(n.li,{children:"Error handling"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Basic Setup"}),": Install Whisper and test speech recognition with audio files"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Processing"}),": Implement real-time audio capture and recognition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Command Mapping"}),": Create a command parser for navigation tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS Integration"}),": Integrate voice commands with a simple ROS 2 navigation system"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This week we explored voice-to-action systems using OpenAI Whisper for speech recognition. We learned how to process audio input, recognize speech commands, parse natural language, and integrate with ROS 2 systems. Voice-to-action technology provides an intuitive interface for human-robot interaction, enabling robots to understand and respond to natural language commands."}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.ros.org/en/humble/",children:"ROS 2 Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://pyaudio.readthedocs.io/",children:"PyAudio Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2103.13233",children:"Speech Recognition in Robotics"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);