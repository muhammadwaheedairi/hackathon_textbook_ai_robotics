"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1948],{2115:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-17-whisper-speech-recognition","title":"Whisper Speech Recognition","description":"Overview","source":"@site/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-17-whisper-speech-recognition.md","sourceDirName":"module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper","slug":"/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-17-whisper-speech-recognition","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-17-whisper-speech-recognition","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadwaheedairi/hackathon_textbook_ai_robotics/edit/main/my-website/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-17-whisper-speech-recognition.md","tags":[],"version":"current","sidebarPosition":17,"frontMatter":{"title":"Whisper Speech Recognition","sidebar_label":"Chapter 17: Whisper Speech Recognition","sidebar_position":17},"sidebar":"tutorialSidebar","previous":{"title":"Module 4 \u2014 Vision-Language-Action (VLA)","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/"},"next":{"title":"Chapter 18: Voice ROS 2 Integration","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-18-voice-ros2-integration"}}');var t=n(4848),s=n(8453);const a={title:"Whisper Speech Recognition",sidebar_label:"Chapter 17: Whisper Speech Recognition",sidebar_position:17},r="Chapter 17: Whisper Speech Recognition",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"OpenAI Whisper for Speech Recognition",id:"openai-whisper-for-speech-recognition",level:2},{value:"Whisper Model Variants",id:"whisper-model-variants",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Basic Whisper Usage",id:"basic-whisper-usage",level:3},{value:"Real-Time Voice Recognition",id:"real-time-voice-recognition",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Audio Stream Processing",id:"audio-stream-processing",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"What&#39;s Next",id:"whats-next",level:2}];function d(e){const i={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.header,{children:(0,t.jsx)(i.h1,{id:"chapter-17-whisper-speech-recognition",children:"Chapter 17: Whisper Speech Recognition"})}),"\n",(0,t.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(i.p,{children:"This chapter introduces OpenAI Whisper for speech recognition in robotics applications. You'll learn how to implement voice-to-action systems that enable robots to understand and respond to natural language commands."}),"\n",(0,t.jsx)(i.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(i.admonition,{title:"Learning Objectives",type:"info",children:[(0,t.jsx)(i.p,{children:"By the end of this chapter, you will be able to:"}),(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Understand Whisper's architecture and capabilities"}),"\n",(0,t.jsx)(i.li,{children:"Install and configure Whisper for real-time speech recognition"}),"\n",(0,t.jsx)(i.li,{children:"Process audio input and convert speech to text"}),"\n",(0,t.jsx)(i.li,{children:"Implement voice activity detection for efficient processing"}),"\n"]})]}),"\n",(0,t.jsx)(i.h2,{id:"openai-whisper-for-speech-recognition",children:"OpenAI Whisper for Speech Recognition"}),"\n",(0,t.jsx)(i.p,{children:"OpenAI Whisper is a state-of-the-art speech recognition model that provides high accuracy across multiple languages, handles various accents and speaking styles, works well in noisy environments, supports real-time and batch processing, and is available as an open-source model."}),"\n",(0,t.jsx)(i.h3,{id:"whisper-model-variants",children:"Whisper Model Variants"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"tiny"}),": Fastest, least accurate (76MB)"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"base"}),": Good balance of speed and accuracy (145MB)"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"small"}),": Better accuracy, moderate speed (484MB)"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"medium"}),": High accuracy, slower (1.5GB)"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.strong,{children:"large"}),": Highest accuracy, slowest (3.0GB)"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-bash",children:"pip install openai-whisper\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"})}),"\n",(0,t.jsx)(i.h3,{id:"basic-whisper-usage",children:"Basic Whisper Usage"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-python",children:'import whisper\n\nmodel = whisper.load_model("small")\n\nresult = model.transcribe("command.wav")\nprint(result["text"])\n'})}),"\n",(0,t.jsx)(i.h2,{id:"real-time-voice-recognition",children:"Real-Time Voice Recognition"}),"\n",(0,t.jsx)(i.p,{children:"For real-time voice recognition, we need to capture audio from microphone, process audio in chunks, handle streaming input efficiently, and filter out background noise."}),"\n",(0,t.jsx)(i.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,t.jsx)(i.h3,{id:"audio-stream-processing",children:"Audio Stream Processing"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-python",children:'import pyaudio\nimport numpy as np\nimport queue\nimport threading\nimport whisper\nimport torch\n\nclass VoiceToAction:\n    def __init__(self, model_size="small"):\n        self.model = whisper.load_model(model_size)\n        self.audio_queue = queue.Queue()\n\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 16000\n        self.chunk = 1024\n\n        self.audio = pyaudio.PyAudio()\n\n    def start_listening(self):\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        threading.Thread(target=self.record_audio, args=(stream,), daemon=True).start()\n\n    def record_audio(self, stream):\n        while True:\n            data = stream.read(self.chunk)\n            self.audio_queue.put(data)\n'})}),"\n",(0,t.jsx)(i.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(i.p,{children:"OpenAI Whisper provides robust speech recognition capabilities for robotics applications. Its multi-language support, noise tolerance, and open-source availability make it ideal for implementing voice-to-action systems that enable natural human-robot interaction."}),"\n",(0,t.jsx)(i.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsx)(i.admonition,{title:"Key Takeaways",type:"tip",children:(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Whisper offers multiple model sizes for different accuracy-speed tradeoffs"}),"\n",(0,t.jsx)(i.li,{children:"Real-time audio processing requires efficient streaming and buffering"}),"\n",(0,t.jsx)(i.li,{children:"Voice activity detection reduces unnecessary processing"}),"\n",(0,t.jsx)(i.li,{children:"Proper audio preprocessing improves recognition accuracy"}),"\n"]})}),"\n",(0,t.jsx)(i.h2,{id:"whats-next",children:"What's Next"}),"\n",(0,t.jsx)(i.p,{children:"In the next chapter, we'll explore integrating Whisper with ROS 2, learning how to map recognized voice commands to robot actions for complete voice-to-action systems."})]})}function h(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>r});var o=n(6540);const t={},s=o.createContext(t);function a(e){const i=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:i},e.children)}}}]);