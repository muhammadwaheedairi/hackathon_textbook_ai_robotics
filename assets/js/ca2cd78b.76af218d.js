"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9147],{7526:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-1-robotic-nervous-system/week-1-introduction-to-physical-ai","title":"Introduction to Physical AI and Sensors","description":"Learning Objectives","source":"@site/docs/module-1-robotic-nervous-system/week-1-introduction-to-physical-ai.md","sourceDirName":"module-1-robotic-nervous-system","slug":"/module-1-robotic-nervous-system/week-1-introduction-to-physical-ai","permalink":"/hackathon_textbook_ai_robotics/docs/module-1-robotic-nervous-system/week-1-introduction-to-physical-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadwaheedairi/hackathon_textbook_ai_robotics/edit/main/my-website/docs/module-1-robotic-nervous-system/week-1-introduction-to-physical-ai.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to Physical AI and Sensors","sidebar_label":"Week 1: Introduction to Physical AI and Sensors","sidebar_position":1},"sidebar":"textbookSidebar","previous":{"title":"Quick Start Guide","permalink":"/hackathon_textbook_ai_robotics/docs/quickstart"},"next":{"title":"Week 2: ROS 2 Fundamentals \u2014 Nodes, Topics, Services, Packages","permalink":"/hackathon_textbook_ai_robotics/docs/module-1-robotic-nervous-system/week-2-ros-2-fundamentals"}}');var r=i(4848),o=i(8453);const t={title:"Introduction to Physical AI and Sensors",sidebar_label:"Week 1: Introduction to Physical AI and Sensors",sidebar_position:1},a="Introduction to Physical AI and Sensors",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Key Characteristics of Physical AI",id:"key-characteristics-of-physical-ai",level:3},{value:"Theory",id:"theory",level:2},{value:"Sensors in Robotics",id:"sensors-in-robotics",level:3},{value:"Sensor Categories",id:"sensor-categories",level:4},{value:"LIDAR Sensors",id:"lidar-sensors",level:3},{value:"LIDAR Working Principles",id:"lidar-working-principles",level:4},{value:"Types of LIDAR Sensors",id:"types-of-lidar-sensors",level:4},{value:"LIDAR Specifications and Parameters",id:"lidar-specifications-and-parameters",level:4},{value:"LIDAR Applications in Robotics",id:"lidar-applications-in-robotics",level:4},{value:"Advantages and Limitations",id:"advantages-and-limitations",level:4},{value:"IMU Sensors",id:"imu-sensors",level:3},{value:"IMU Components",id:"imu-components",level:4},{value:"IMU Applications in Robotics",id:"imu-applications-in-robotics",level:4},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Common Sensor Message Types",id:"common-sensor-message-types",level:4},{value:"Code Examples",id:"code-examples",level:2},{value:"LIDAR Data Subscription",id:"lidar-data-subscription",level:3},{value:"IMU Data Processing",id:"imu-data-processing",level:3},{value:"Basic Sensor Fusion Example",id:"basic-sensor-fusion-example",level:3},{value:"Visualization of Sensor Data",id:"visualization-of-sensor-data",level:3},{value:"Exercises",id:"exercises",level:2},{value:"References",id:"references",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"introduction-to-physical-ai-and-sensors",children:"Introduction to Physical AI and Sensors"})}),"\n",(0,r.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(e.p,{children:"By the end of this week, students will be able to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Define Physical AI and its applications in robotics"}),"\n",(0,r.jsx)(e.li,{children:"Identify and describe the main types of sensors used in robotics"}),"\n",(0,r.jsx)(e.li,{children:"Understand the principles of LIDAR and IMU sensors"}),"\n",(0,r.jsx)(e.li,{children:"Explain how sensors enable robot perception in real-world environments"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(e.p,{children:"This week introduces the fundamental concepts of Physical AI and the essential sensors used in robotics applications. Physical AI refers to the integration of artificial intelligence with physical systems, enabling robots to perceive, reason, and act in the real world."}),"\n",(0,r.jsx)(e.p,{children:"Physical AI represents a paradigm shift in robotics, where artificial intelligence algorithms are tightly integrated with physical systems to create intelligent machines capable of interacting with the real world. Unlike traditional AI systems that operate purely in digital domains, Physical AI systems must handle the complexities of real-world perception, uncertainty, and physical constraints."}),"\n",(0,r.jsx)(e.h3,{id:"key-characteristics-of-physical-ai",children:"Key Characteristics of Physical AI"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Embodied Intelligence"}),": AI algorithms are designed specifically for physical interaction"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Real-time Processing"}),": Systems must respond to environmental changes in real-time"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensor Integration"}),": Multiple sensor modalities work together to perceive the environment"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Uncertainty Management"}),": Systems must handle noisy sensor data and uncertain environments"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Safety Considerations"}),": Physical systems must operate safely in human environments"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"theory",children:"Theory"}),"\n",(0,r.jsx)(e.h3,{id:"sensors-in-robotics",children:"Sensors in Robotics"}),"\n",(0,r.jsx)(e.p,{children:"Robots rely on various sensors to perceive their environment and make informed decisions. The quality and integration of sensor data directly impacts the robot's ability to navigate, interact, and perform tasks effectively."}),"\n",(0,r.jsx)(e.h4,{id:"sensor-categories",children:"Sensor Categories"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Proprioceptive Sensors"}),": Measure internal robot state (joint angles, motor currents)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Exteroceptive Sensors"}),": Measure external environment (cameras, LIDAR, IMU)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Interoceptive Sensors"}),": Measure internal robot conditions (temperature, power)"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"lidar-sensors",children:"LIDAR Sensors"}),"\n",(0,r.jsx)(e.p,{children:"Light Detection and Ranging (LIDAR) sensors are optical remote sensing devices that measure properties of scattered light to determine the range of distant objects. LIDAR sensors emit laser pulses and measure the time it takes for the light to return after reflecting off objects. This enables precise distance measurements and the creation of detailed 3D maps of the environment."}),"\n",(0,r.jsx)(e.h4,{id:"lidar-working-principles",children:"LIDAR Working Principles"}),"\n",(0,r.jsx)(e.p,{children:"LIDAR operates on the principle of time-of-flight measurement:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Emission"}),": The sensor emits a laser pulse at the speed of light (c \u2248 3\xd710\u2078 m/s)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Reflection"}),": The pulse reflects off objects in the environment"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Detection"}),": The sensor detects the returning pulse"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Calculation"}),": Distance is calculated using the formula: distance = (speed of light \xd7 time delay) / 2"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"The factor of 2 accounts for the round trip of the laser pulse."}),"\n",(0,r.jsx)(e.h4,{id:"types-of-lidar-sensors",children:"Types of LIDAR Sensors"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Mechanical LIDAR"}),": Rotating mirrors to scan the environment (e.g., Velodyne HDL-64)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Solid-state LIDAR"}),": No moving parts, using optical phased arrays or flash LIDAR"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Coherent LIDAR"}),": Uses frequency modulation for velocity measurements"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Direct Detection LIDAR"}),": Measures only the intensity of returned light"]}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"lidar-specifications-and-parameters",children:"LIDAR Specifications and Parameters"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Range"}),": Detection distance (typically 10-300m)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Accuracy"}),": Measurement precision (typically 1-3cm)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Resolution"}),": Angular resolution between measurements (0.1\xb0-0.5\xb0)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Field of View"}),": Angular coverage (horizontal and vertical)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Scan Rate"}),": Frequency of complete scans (5-20Hz)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Data Rate"}),": Points generated per second (thousands to millions)"]}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"lidar-applications-in-robotics",children:"LIDAR Applications in Robotics"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Environment mapping and localization (SLAM)"}),"\n",(0,r.jsx)(e.li,{children:"Obstacle detection and collision avoidance"}),"\n",(0,r.jsx)(e.li,{children:"3D scene reconstruction and modeling"}),"\n",(0,r.jsx)(e.li,{children:"Navigation and path planning"}),"\n",(0,r.jsx)(e.li,{children:"Object detection and classification"}),"\n",(0,r.jsx)(e.li,{children:"Precision agriculture and autonomous vehicles"}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"advantages-and-limitations",children:"Advantages and Limitations"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Advantages:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"High accuracy and precision"}),"\n",(0,r.jsx)(e.li,{children:"Works in various lighting conditions"}),"\n",(0,r.jsx)(e.li,{children:"Provides dense 3D point cloud data"}),"\n",(0,r.jsx)(e.li,{children:"Effective for distance measurement"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Limitations:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Performance affected by weather (fog, rain, snow)"}),"\n",(0,r.jsx)(e.li,{children:"Expensive compared to other sensors"}),"\n",(0,r.jsx)(e.li,{children:"Can be affected by highly reflective surfaces"}),"\n",(0,r.jsx)(e.li,{children:"Limited resolution for fine details"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"imu-sensors",children:"IMU Sensors"}),"\n",(0,r.jsx)(e.p,{children:"Inertial Measurement Units (IMUs) combine accelerometers, gyroscopes, and sometimes magnetometers to measure the robot's orientation, velocity, and gravitational forces."}),"\n",(0,r.jsx)(e.h4,{id:"imu-components",children:"IMU Components"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Accelerometer"}),": Measures linear acceleration"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Gyroscope"}),": Measures angular velocity"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Magnetometer"}),": Measures magnetic field orientation (compass)"]}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"imu-applications-in-robotics",children:"IMU Applications in Robotics"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Robot pose estimation"}),"\n",(0,r.jsx)(e.li,{children:"Motion tracking and control"}),"\n",(0,r.jsx)(e.li,{children:"Stabilization systems"}),"\n",(0,r.jsx)(e.li,{children:"Dead reckoning navigation"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,r.jsx)(e.p,{children:"The Robot Operating System 2 (ROS 2) provides standardized interfaces for sensor integration, making it easier to work with LIDAR and IMU sensors in robotics applications."}),"\n",(0,r.jsx)(e.h4,{id:"common-sensor-message-types",children:"Common Sensor Message Types"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"sensor_msgs/LaserScan"}),": For LIDAR data"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"sensor_msgs/Imu"}),": For IMU data"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"sensor_msgs/PointCloud2"}),": For 3D point cloud data"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,r.jsx)(e.h3,{id:"lidar-data-subscription",children:"LIDAR Data Subscription"}),"\n",(0,r.jsx)(e.p,{children:"Here's a basic ROS 2 Python node that subscribes to LIDAR data:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nimport numpy as np\n\nclass LidarSubscriber(Node):\n    def __init__(self):\n        super().__init__('lidar_subscriber')\n        self.subscription = self.create_subscription(\n            LaserScan,\n            '/scan',  # Topic name - varies by robot/LIDAR\n            self.lidar_callback,\n            10)  # QoS queue size\n        self.subscription  # Prevent unused variable warning\n\n        self.get_logger().info('LIDAR Subscriber node initialized')\n\n    def lidar_callback(self, msg):\n        \"\"\"Process incoming LIDAR data\"\"\"\n        # Extract key information from the LaserScan message\n        ranges = np.array(msg.ranges)\n        angle_min = msg.angle_min\n        angle_max = msg.angle_max\n        angle_increment = msg.angle_increment\n\n        # Filter out invalid measurements (inf or nan)\n        valid_ranges = ranges[np.isfinite(ranges)]\n\n        # Find minimum distance (closest obstacle)\n        if len(valid_ranges) > 0:\n            min_distance = np.min(valid_ranges)\n            self.get_logger().info(f'Minimum distance: {min_distance:.2f}m')\n\n        # Calculate distances within a specific angle range (e.g., front of robot)\n        front_angle_start = int(len(ranges) / 2 - len(ranges) / 10)  # -10% of FOV\n        front_angle_end = int(len(ranges) / 2 + len(ranges) / 10)    # +10% of FOV\n        front_distances = ranges[front_angle_start:front_angle_end]\n        front_valid = front_distances[np.isfinite(front_distances)]\n\n        if len(front_valid) > 0:\n            front_min = np.min(front_valid)\n            self.get_logger().info(f'Front minimum distance: {front_min:.2f}m')\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    lidar_subscriber = LidarSubscriber()\n\n    try:\n        rclpy.spin(lidar_subscriber)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        lidar_subscriber.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(e.h3,{id:"imu-data-processing",children:"IMU Data Processing"}),"\n",(0,r.jsx)(e.p,{children:"Here's a ROS 2 Python node for processing IMU data:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3\nimport math\n\nclass ImuSubscriber(Node):\n    def __init__(self):\n        super().__init__('imu_subscriber')\n        self.subscription = self.create_subscription(\n            Imu,\n            '/imu/data',  # Topic name - varies by IMU setup\n            self.imu_callback,\n            10)\n        self.subscription\n\n        self.get_logger().info('IMU Subscriber node initialized')\n\n    def imu_callback(self, msg):\n        \"\"\"Process incoming IMU data\"\"\"\n        # Extract orientation (quaternion)\n        orientation = msg.orientation\n        roll, pitch, yaw = self.quaternion_to_euler(\n            orientation.x, orientation.y, orientation.z, orientation.w)\n\n        # Extract angular velocity\n        angular_velocity = msg.angular_velocity\n        # Extract linear acceleration\n        linear_acceleration = msg.linear_acceleration\n\n        # Log key values\n        self.get_logger().info(\n            f'Orientation - Roll: {math.degrees(roll):.2f}\xb0, '\n            f'Pitch: {math.degrees(pitch):.2f}\xb0, '\n            f'Yaw: {math.degrees(yaw):.2f}\xb0'\n        )\n        self.get_logger().info(\n            f'Angular Vel - X: {angular_velocity.x:.2f}, '\n            f'Y: {angular_velocity.y:.2f}, '\n            f'Z: {angular_velocity.z:.2f}'\n        )\n        self.get_logger().info(\n            f'Linear Accel - X: {linear_acceleration.x:.2f}, '\n            f'Y: {linear_acceleration.y:.2f}, '\n            f'Z: {linear_acceleration.z:.2f}'\n        )\n\n    def quaternion_to_euler(self, x, y, z, w):\n        \"\"\"Convert quaternion to Euler angles (roll, pitch, yaw)\"\"\"\n        # Roll (x-axis rotation)\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = math.atan2(sinr_cosp, cosr_cosp)\n\n        # Pitch (y-axis rotation)\n        sinp = 2 * (w * y - z * x)\n        if abs(sinp) >= 1:\n            pitch = math.copysign(math.pi / 2, sinp)  # Use 90 degrees if out of range\n        else:\n            pitch = math.asin(sinp)\n\n        # Yaw (z-axis rotation)\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = math.atan2(siny_cosp, cosy_cosp)\n\n        return roll, pitch, yaw\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    imu_subscriber = ImuSubscriber()\n\n    try:\n        rclpy.spin(imu_subscriber)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        imu_subscriber.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(e.h3,{id:"basic-sensor-fusion-example",children:"Basic Sensor Fusion Example"}),"\n",(0,r.jsx)(e.p,{children:"Here's a simple example of combining LIDAR and IMU data:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Imu\nfrom geometry_msgs.msg import Twist\nimport numpy as np\nimport math\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__(\'sensor_fusion_node\')\n\n        # Subscribe to LIDAR and IMU data\n        self.lidar_subscription = self.create_subscription(\n            LaserScan, \'/scan\', self.lidar_callback, 10)\n        self.imu_subscription = self.create_subscription(\n            Imu, \'/imu/data\', self.imu_callback, 10)\n\n        # Publisher for robot commands\n        self.cmd_vel_publisher = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Initialize sensor data storage\n        self.latest_lidar_data = None\n        self.latest_imu_data = None\n\n        self.get_logger().info(\'Sensor Fusion Node initialized\')\n\n    def lidar_callback(self, msg):\n        """Store latest LIDAR data"""\n        self.latest_lidar_data = msg\n\n        # Process LIDAR data for obstacle detection\n        if self.process_lidar_for_obstacles():\n            self.get_logger().warn(\'Obstacle detected! Stopping robot.\')\n            self.stop_robot()\n\n    def imu_callback(self, msg):\n        """Store latest IMU data"""\n        self.latest_imu_data = msg\n\n        # Process IMU data for orientation\n        orientation = msg.orientation\n        roll, pitch, yaw = self.quaternion_to_euler(\n            orientation.x, orientation.y, orientation.z, orientation.w)\n\n        # Log orientation (could be used for navigation)\n        self.get_logger().info(f\'Robot orientation: Yaw={math.degrees(yaw):.2f}\xb0\')\n\n    def process_lidar_for_obstacles(self):\n        """Check if there are obstacles in front of the robot"""\n        if self.latest_lidar_data is None:\n            return False\n\n        ranges = np.array(self.latest_lidar_data.ranges)\n        valid_ranges = ranges[np.isfinite(ranges)]\n\n        if len(valid_ranges) == 0:\n            return False\n\n        # Check front-facing range (middle 20% of the scan)\n        center_idx = len(ranges) // 2\n        front_range = ranges[center_idx - len(ranges)//10:center_idx + len(ranges)//10]\n        front_valid = front_range[np.isfinite(front_range)]\n\n        if len(front_valid) > 0:\n            min_front_distance = np.min(front_valid)\n            # If obstacle is closer than 1 meter, consider it a threat\n            return min_front_distance < 1.0\n\n        return False\n\n    def stop_robot(self):\n        """Send stop command to robot"""\n        stop_cmd = Twist()\n        stop_cmd.linear.x = 0.0\n        stop_cmd.angular.z = 0.0\n        self.cmd_vel_publisher.publish(stop_cmd)\n\n    def quaternion_to_euler(self, x, y, z, w):\n        """Convert quaternion to Euler angles"""\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = math.atan2(sinr_cosp, cosr_cosp)\n\n        sinp = 2 * (w * y - z * x)\n        if abs(sinp) >= 1:\n            pitch = math.copysign(math.pi / 2, sinp)\n        else:\n            pitch = math.asin(sinp)\n\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = math.atan2(siny_cosp, cosy_cosp)\n\n        return roll, pitch, yaw\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    sensor_fusion_node = SensorFusionNode()\n\n    try:\n        rclpy.spin(sensor_fusion_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        sensor_fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(e.h3,{id:"visualization-of-sensor-data",children:"Visualization of Sensor Data"}),"\n",(0,r.jsx)(e.p,{children:"For visualizing sensor data, you can use RViz2 which comes with ROS 2. Here's a simple example of publishing data for visualization:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom visualization_msgs.msg import Marker\nfrom geometry_msgs.msg import Point\nimport math\n\nclass SensorVisualizationNode(Node):\n    def __init__(self):\n        super().__init__('sensor_visualization_node')\n\n        self.lidar_subscription = self.create_subscription(\n            LaserScan, '/scan', self.lidar_callback, 10)\n\n        # Publisher for visualization markers\n        self.marker_publisher = self.create_publisher(Marker, '/lidar_points', 10)\n\n        self.get_logger().info('Sensor Visualization Node initialized')\n\n    def lidar_callback(self, msg):\n        \"\"\"Convert LIDAR scan to visualization markers\"\"\"\n        # Create a marker to visualize LIDAR points\n        marker = Marker()\n        marker.header = msg.header\n        marker.ns = \"lidar_points\"\n        marker.id = 0\n        marker.type = Marker.POINTS\n        marker.action = Marker.ADD\n\n        # Set marker scale (point size)\n        marker.scale.x = 0.05  # Width\n        marker.scale.y = 0.05  # Height\n        marker.scale.z = 0.05  # Depth\n\n        # Set marker color (red)\n        marker.color.r = 1.0\n        marker.color.g = 0.0\n        marker.color.b = 0.0\n        marker.color.a = 1.0  # Alpha (opacity)\n\n        # Convert LIDAR ranges to 3D points\n        angle = msg.angle_min\n        for i, range_val in enumerate(msg.ranges):\n            if not math.isinf(range_val) and not math.isnan(range_val):\n                # Calculate x, y coordinates from polar coordinates\n                x = range_val * math.cos(angle)\n                y = range_val * math.sin(angle)\n\n                point = Point()\n                point.x = x\n                point.y = y\n                point.z = 0.0  # Z is 0 for 2D LIDAR\n\n                marker.points.append(point)\n\n            angle += msg.angle_increment\n\n        # Publish the marker\n        self.marker_publisher.publish(marker)\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    vis_node = SensorVisualizationNode()\n\n    try:\n        rclpy.spin(vis_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vis_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Research and compare different LIDAR sensors available in the market (Hokuyo, Velodyne, Ouster, etc.)"}),"\n",(0,r.jsx)(e.li,{children:"Investigate the specifications of common IMU sensors (MPU6050, BNO055, etc.)"}),"\n",(0,r.jsx)(e.li,{children:"Explore how sensor fusion combines LIDAR and IMU data for improved perception"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Thrun, S., Burgard, W., & Fox, D. (2005). Probabilistic Robotics"}),"\n",(0,r.jsx)(e.li,{children:"Siciliano, B., & Khatib, O. (2016). Springer Handbook of Robotics"}),"\n",(0,r.jsxs)(e.li,{children:["ROS 2 Documentation: ",(0,r.jsx)(e.a,{href:"https://docs.ros.org/en/humble/",children:"https://docs.ros.org/en/humble/"})]}),"\n",(0,r.jsxs)(e.li,{children:["LIDAR Sensor Guide: ",(0,r.jsx)(e.a,{href:"https://www.osrfoundation.org/",children:"https://www.osrfoundation.org/"})]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>a});var s=i(6540);const r={},o=s.createContext(r);function t(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);