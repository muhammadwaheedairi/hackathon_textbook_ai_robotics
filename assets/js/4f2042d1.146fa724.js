"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7204],{7711:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-18-voice-ros2-integration","title":"Voice ROS 2 Integration","description":"Overview","source":"@site/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-18-voice-ros2-integration.md","sourceDirName":"module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper","slug":"/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-18-voice-ros2-integration","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-18-voice-ros2-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadwaheedairi/hackathon_textbook_ai_robotics/edit/main/my-website/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-18-voice-ros2-integration.md","tags":[],"version":"current","sidebarPosition":18,"frontMatter":{"title":"Voice ROS 2 Integration","sidebar_label":"Chapter 18: Voice ROS 2 Integration","sidebar_position":18},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 17: Whisper Speech Recognition","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-17-whisper-speech-recognition"},"next":{"title":"Chapter 19: LLM Cognitive Planning","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-10-cognitive-planning/chapter-19-llm-cognitive-planning"}}');var a=i(4848),o=i(8453);const r={title:"Voice ROS 2 Integration",sidebar_label:"Chapter 18: Voice ROS 2 Integration",sidebar_position:18},s="Chapter 18: Voice ROS 2 Integration",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Natural Language Understanding",id:"natural-language-understanding",level:2},{value:"Command Parsing",id:"command-parsing",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"ROS 2 Node Structure",id:"ros-2-node-structure",level:3},{value:"Voice Command Vocabulary",id:"voice-command-vocabulary",level:2},{value:"Basic Navigation Commands",id:"basic-navigation-commands",level:3},{value:"Manipulation Commands",id:"manipulation-commands",level:3},{value:"System Commands",id:"system-commands",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"What&#39;s Next",id:"whats-next",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-18-voice-ros-2-integration",children:"Chapter 18: Voice ROS 2 Integration"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This chapter explores integrating Whisper-based speech recognition with ROS 2 for robot control. You'll learn how to map voice commands to robot actions, creating intuitive voice-controlled robotics systems."}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)(n.admonition,{title:"Learning Objectives",type:"info",children:[(0,a.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Integrate Whisper with ROS 2 control systems"}),"\n",(0,a.jsx)(n.li,{children:"Parse natural language commands into robot actions"}),"\n",(0,a.jsx)(n.li,{children:"Implement command vocabularies for robot control"}),"\n",(0,a.jsx)(n.li,{children:"Handle ambiguous commands and error cases"}),"\n"]})]}),"\n",(0,a.jsx)(n.h2,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,a.jsx)(n.p,{children:"Convert recognized text into structured commands by extracting action verbs (move, pick, place, etc.), identifying objects and locations, parsing numerical parameters, and handling complex multi-step commands."}),"\n",(0,a.jsx)(n.h3,{id:"command-parsing",children:"Command Parsing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import re\n\nclass CommandParser:\n    def __init__(self):\n        self.move_patterns = [\n            r'move to (.+)',\n            r'go to (.+)',\n            r'navigate to (.+)'\n        ]\n\n        self.pick_patterns = [\n            r'pick up the (.+)',\n            r'grab the (.+)',\n            r'take the (.+)'\n        ]\n\n    def parse_command(self, text):\n        text = text.lower().strip()\n\n        for pattern in self.move_patterns:\n            match = re.search(pattern, text)\n            if match:\n                return {'action': 'move', 'target': match.group(1)}\n\n        for pattern in self.pick_patterns:\n            match = re.search(pattern, text)\n            if match:\n                return {'action': 'pick', 'object': match.group(1)}\n\n        return {'action': 'unknown', 'raw': text}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,a.jsx)(n.h3,{id:"ros-2-node-structure",children:"ROS 2 Node Structure"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom sensor_msgs.msg import AudioData\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__('voice_command_node')\n\n        self.move_pub = self.create_publisher(Pose, 'move_command', 10)\n        self.action_pub = self.create_publisher(String, 'action_command', 10)\n\n        self.audio_sub = self.create_subscription(\n            AudioData, 'audio_input', self.audio_callback, 10)\n\n        self.timer = self.create_timer(0.1, self.process_audio)\n\n        self.whisper_model = whisper.load_model(\"small\")\n\n    def audio_callback(self, msg):\n        audio_array = np.frombuffer(msg.data, dtype=np.int16)\n        result = self.whisper_model.transcribe(audio_array)\n        command_text = result[\"text\"]\n\n        self.execute_command(command_text)\n\n    def execute_command(self, command_text):\n        parser = CommandParser()\n        parsed_command = parser.parse_command(command_text)\n\n        if parsed_command['action'] == 'move':\n            self.send_move_command(parsed_command['target'])\n        elif parsed_command['action'] == 'pick':\n            self.send_pick_command(parsed_command['object'])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"voice-command-vocabulary",children:"Voice Command Vocabulary"}),"\n",(0,a.jsx)(n.h3,{id:"basic-navigation-commands",children:"Basic Navigation Commands"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:'"Go to the kitchen"'}),"\n",(0,a.jsx)(n.li,{children:'"Move to the table"'}),"\n",(0,a.jsx)(n.li,{children:'"Navigate to the charging station"'}),"\n",(0,a.jsx)(n.li,{children:'"Return to base"'}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"manipulation-commands",children:"Manipulation Commands"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:'"Pick up the red cup"'}),"\n",(0,a.jsx)(n.li,{children:'"Place the book on the shelf"'}),"\n",(0,a.jsx)(n.li,{children:'"Open the door"'}),"\n",(0,a.jsx)(n.li,{children:'"Close the drawer"'}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"system-commands",children:"System Commands"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:'"Stop" or "Halt"'}),"\n",(0,a.jsx)(n.li,{children:'"Pause"'}),"\n",(0,a.jsx)(n.li,{children:'"Resume"'}),"\n",(0,a.jsx)(n.li,{children:'"Status"'}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"Integrating Whisper with ROS 2 enables voice-controlled robotics systems. Command parsing translates natural language into structured robot actions, while ROS 2 integration provides the communication infrastructure for executing commands on physical robots."}),"\n",(0,a.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,a.jsx)(n.admonition,{title:"Key Takeaways",type:"tip",children:(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Command parsing extracts structured actions from natural language"}),"\n",(0,a.jsx)(n.li,{children:"ROS 2 integration enables seamless voice-to-action pipelines"}),"\n",(0,a.jsx)(n.li,{children:"Well-defined command vocabularies improve recognition accuracy"}),"\n",(0,a.jsx)(n.li,{children:"Error handling and confirmation improve system reliability"}),"\n"]})}),"\n",(0,a.jsx)(n.h2,{id:"whats-next",children:"What's Next"}),"\n",(0,a.jsx)(n.p,{children:"In the next chapter, we'll explore cognitive planning with LLMs, learning how to use large language models for high-level robot task planning and reasoning."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var t=i(6540);const a={},o=t.createContext(a);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);