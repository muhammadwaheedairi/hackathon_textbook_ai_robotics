"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4859],{160:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vision-language-action/week-10-cognitive-planning","title":"Cognitive Planning - LLMs Translating Natural Language to ROS 2 Actions","description":"Introduction","source":"@site/docs/module-4-vision-language-action/week-10-cognitive-planning.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/week-10-cognitive-planning","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-10-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadwaheedairi/hackathon_textbook_ai_robotics/edit/main/my-website/docs/module-4-vision-language-action/week-10-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"Cognitive Planning - LLMs Translating Natural Language to ROS 2 Actions","sidebar_label":"Week 10 - Cognitive Planning with LLMs","sidebar_position":10},"sidebar":"textbookSidebar","previous":{"title":"Week 9 - Voice-to-Action with OpenAI Whisper","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper"},"next":{"title":"Week 11-13 - Capstone Autonomous Humanoid","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-11-13-capstone-autonomous-humanoid"}}');var o=i(4848),s=i(8453);const a={title:"Cognitive Planning - LLMs Translating Natural Language to ROS 2 Actions",sidebar_label:"Week 10 - Cognitive Planning with LLMs",sidebar_position:10},l="Week 10: Cognitive Planning - LLMs Translating Natural Language to ROS 2 Actions",r={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"1. Introduction to Cognitive Robotics with LLMs",id:"1-introduction-to-cognitive-robotics-with-llms",level:2},{value:"1.1 What is Cognitive Robotics?",id:"11-what-is-cognitive-robotics",level:3},{value:"1.2 Role of LLMs in Cognitive Robotics",id:"12-role-of-llms-in-cognitive-robotics",level:3},{value:"1.3 Architecture of LLM-Based Cognitive Systems",id:"13-architecture-of-llm-based-cognitive-systems",level:3},{value:"2. LLM Integration for Robotics",id:"2-llm-integration-for-robotics",level:2},{value:"2.1 Choosing the Right LLM",id:"21-choosing-the-right-llm",level:3},{value:"2.2 Popular LLM Options for Robotics",id:"22-popular-llm-options-for-robotics",level:3},{value:"2.3 API Integration Patterns",id:"23-api-integration-patterns",level:3},{value:"3. Prompt Engineering for Robotics",id:"3-prompt-engineering-for-robotics",level:2},{value:"3.1 System Prompt Design",id:"31-system-prompt-design",level:3},{value:"3.2 Few-Shot Learning Examples",id:"32-few-shot-learning-examples",level:3},{value:"3.3 Context-Aware Prompting",id:"33-context-aware-prompting",level:3},{value:"4. Action Planning and Execution",id:"4-action-planning-and-execution",level:2},{value:"4.1 Action Representation",id:"41-action-representation",level:3},{value:"4.2 Plan Validation",id:"42-plan-validation",level:3},{value:"4.3 ROS 2 Action Integration",id:"43-ros-2-action-integration",level:3},{value:"5. Handling Ambiguity and Errors",id:"5-handling-ambiguity-and-errors",level:2},{value:"5.1 Ambiguity Detection",id:"51-ambiguity-detection",level:3},{value:"5.2 Clarification Strategies",id:"52-clarification-strategies",level:3},{value:"5.3 Error Recovery",id:"53-error-recovery",level:3},{value:"6. Safety and Validation",id:"6-safety-and-validation",level:2},{value:"6.1 Safety Constraints",id:"61-safety-constraints",level:3},{value:"6.2 Plan Verification",id:"62-plan-verification",level:3},{value:"6.3 Human-in-the-Loop",id:"63-human-in-the-loop",level:3},{value:"7. Performance Optimization",id:"7-performance-optimization",level:2},{value:"7.1 Caching Strategies",id:"71-caching-strategies",level:3},{value:"7.2 Local Processing",id:"72-local-processing",level:3},{value:"7.3 Asynchronous Processing",id:"73-asynchronous-processing",level:3},{value:"8. Advanced Cognitive Planning",id:"8-advanced-cognitive-planning",level:2},{value:"8.1 Multi-Modal Integration",id:"81-multi-modal-integration",level:3},{value:"8.2 Learning and Adaptation",id:"82-learning-and-adaptation",level:3},{value:"8.3 Collaborative Planning",id:"83-collaborative-planning",level:3},{value:"9. Practical Implementation",id:"9-practical-implementation",level:2},{value:"9.1 Complete Cognitive Planning System",id:"91-complete-cognitive-planning-system",level:3},{value:"10. Testing and Evaluation",id:"10-testing-and-evaluation",level:2},{value:"10.1 Plan Quality Metrics",id:"101-plan-quality-metrics",level:3},{value:"10.2 Human-Robot Interaction Metrics",id:"102-human-robot-interaction-metrics",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"week-10-cognitive-planning---llms-translating-natural-language-to-ros-2-actions",children:"Week 10: Cognitive Planning - LLMs Translating Natural Language to ROS 2 Actions"})}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(e.p,{children:"Welcome to Week 10 of the Vision-Language-Action (VLA) module! This week we'll explore cognitive planning using Large Language Models (LLMs) to translate natural language instructions into executable ROS 2 actions. We'll learn how to leverage the reasoning capabilities of LLMs to create sophisticated robot behaviors that can interpret complex, high-level commands and break them down into specific, executable robot actions."}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this week, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understand the role of LLMs in cognitive robotics"}),"\n",(0,o.jsx)(e.li,{children:"Implement LLM-based natural language understanding for robots"}),"\n",(0,o.jsx)(e.li,{children:"Design prompt engineering strategies for robotics tasks"}),"\n",(0,o.jsx)(e.li,{children:"Translate high-level natural language commands into ROS 2 action sequences"}),"\n",(0,o.jsx)(e.li,{children:"Create robust cognitive planning pipelines that handle ambiguity and errors"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(e.p,{children:"Before starting this week's content, ensure you have:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understanding of ROS 2 fundamentals (Weeks 1-3)"}),"\n",(0,o.jsx)(e.li,{children:"Experience with voice-to-action systems (Week 9)"}),"\n",(0,o.jsx)(e.li,{children:"Basic knowledge of natural language processing"}),"\n",(0,o.jsx)(e.li,{children:"Familiarity with API integration concepts"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"1-introduction-to-cognitive-robotics-with-llms",children:"1. Introduction to Cognitive Robotics with LLMs"}),"\n",(0,o.jsx)(e.h3,{id:"11-what-is-cognitive-robotics",children:"1.1 What is Cognitive Robotics?"}),"\n",(0,o.jsx)(e.p,{children:"Cognitive robotics involves creating robots that can:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understand high-level, natural language commands"}),"\n",(0,o.jsx)(e.li,{children:"Reason about the environment and task requirements"}),"\n",(0,o.jsx)(e.li,{children:"Plan complex sequences of actions"}),"\n",(0,o.jsx)(e.li,{children:"Adapt to unexpected situations"}),"\n",(0,o.jsx)(e.li,{children:"Learn from experience and interaction"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"12-role-of-llms-in-cognitive-robotics",children:"1.2 Role of LLMs in Cognitive Robotics"}),"\n",(0,o.jsx)(e.p,{children:"Large Language Models enhance cognitive robotics by:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Natural Language Understanding"}),": Interpreting human instructions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Reasoning"}),": Planning multi-step actions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Knowledge Integration"}),": Accessing world knowledge"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Context Awareness"}),": Understanding situational context"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Adaptation"}),": Learning from interaction"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"13-architecture-of-llm-based-cognitive-systems",children:"1.3 Architecture of LLM-Based Cognitive Systems"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Input Processing"}),": Natural language command reception"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Understanding"}),": LLM-based semantic analysis"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Planning"}),": Action sequence generation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Execution"}),": ROS 2 command execution"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Feedback"}),": Result interpretation and learning"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"2-llm-integration-for-robotics",children:"2. LLM Integration for Robotics"}),"\n",(0,o.jsx)(e.h3,{id:"21-choosing-the-right-llm",children:"2.1 Choosing the Right LLM"}),"\n",(0,o.jsx)(e.p,{children:"Consider these factors for robotics applications:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Response Time"}),": Real-time vs. batch processing"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Accuracy"}),": Understanding complex commands"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cost"}),": API usage and computational requirements"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Privacy"}),": Handling sensitive data"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Customization"}),": Fine-tuning capabilities"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"22-popular-llm-options-for-robotics",children:"2.2 Popular LLM Options for Robotics"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"OpenAI GPT"}),": High capability, good documentation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Anthropic Claude"}),": Strong reasoning, safety focus"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Google Gemini"}),": Multimodal capabilities"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Open Source Models"}),": Mistral, Llama (for local deployment)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Specialized Models"}),": Fine-tuned for robotics tasks"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"23-api-integration-patterns",children:"2.3 API Integration Patterns"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import openai\nimport json\nfrom typing import Dict, List, Any\n\nclass LLMRobotPlanner:\n    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):\n        openai.api_key = api_key\n        self.model = model\n        self.system_prompt = self._build_system_prompt()\n\n    def _build_system_prompt(self) -> str:\n        return """\n        You are a robotic task planner. Your job is to interpret natural language commands\n        and translate them into structured robot actions for a ROS 2 system.\n\n        Available actions:\n        - move_to(location): Move robot to specified location\n        - pick_object(object_name, location): Pick up an object\n        - place_object(object_name, location): Place an object at location\n        - navigate_to(location): Navigate to location\n        - detect_object(object_type): Detect objects of specified type\n        - wait(duration): Wait for specified duration\n        - report_status(): Report current robot status\n\n        Respond with a JSON object containing:\n        {\n            "action_sequence": [\n                {\n                    "action": "action_name",\n                    "parameters": {"param1": "value1", ...}\n                }\n            ],\n            "reasoning": "Brief explanation of the plan"\n        }\n\n        Be specific about locations and objects. If information is ambiguous,\n        ask for clarification.\n        """\n\n    def plan_task(self, command: str) -> Dict[str, Any]:\n        response = openai.ChatCompletion.create(\n            model=self.model,\n            messages=[\n                {"role": "system", "content": self.system_prompt},\n                {"role": "user", "content": command}\n            ],\n            temperature=0.1  # Low temperature for consistent responses\n        )\n\n        try:\n            result = json.loads(response.choices[0].message[\'content\'])\n            return result\n        except json.JSONDecodeError:\n            # Handle cases where response isn\'t valid JSON\n            return {"action_sequence": [], "reasoning": "Failed to parse response"}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"3-prompt-engineering-for-robotics",children:"3. Prompt Engineering for Robotics"}),"\n",(0,o.jsx)(e.h3,{id:"31-system-prompt-design",children:"3.1 System Prompt Design"}),"\n",(0,o.jsx)(e.p,{children:"Effective system prompts for robotics should include:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Role Definition"}),": Clearly define the LLM's role"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action Vocabulary"}),": List available robot actions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Format Requirements"}),": Specify output format"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Context Information"}),": Provide relevant environment info"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Safety Guidelines"}),": Include safety constraints"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"32-few-shot-learning-examples",children:"3.2 Few-Shot Learning Examples"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'def get_few_shot_examples() -> List[Dict[str, str]]:\n    return [\n        {\n            "role": "user",\n            "content": "Go to the kitchen and bring me a cup of coffee."\n        },\n        {\n            "role": "assistant",\n            "content": json.dumps({\n                "action_sequence": [\n                    {"action": "navigate_to", "parameters": {"location": "kitchen"}},\n                    {"action": "detect_object", "parameters": {"object_type": "cup"}},\n                    {"action": "pick_object", "parameters": {"object_name": "cup", "location": "kitchen counter"}},\n                    {"action": "navigate_to", "parameters": {"location": "coffee machine"}},\n                    {"action": "place_object", "parameters": {"object_name": "cup", "location": "coffee machine tray"}},\n                    {"action": "navigate_to", "parameters": {"location": "your location"}}\n                ],\n                "reasoning": "First navigate to kitchen, detect cup, pick it up, then go to coffee machine to place cup, then return."\n            })\n        },\n        {\n            "role": "user",\n            "content": "Clean the table in the living room."\n        },\n        {\n            "role": "assistant",\n            "content": json.dumps({\n                "action_sequence": [\n                    {"action": "navigate_to", "parameters": {"location": "living room"}},\n                    {"action": "detect_object", "parameters": {"object_type": "debris"}},\n                    {"action": "pick_object", "parameters": {"object_name": "debris", "location": "living room table"}},\n                    {"action": "navigate_to", "parameters": {"location": "trash bin"}},\n                    {"action": "place_object", "parameters": {"object_name": "debris", "location": "trash bin"}},\n                    {"action": "report_status", "parameters": {}}\n                ],\n                "reasoning": "Navigate to living room, detect debris on table, pick up debris, dispose in trash bin, report completion."\n            })\n        }\n    ]\n'})}),"\n",(0,o.jsx)(e.h3,{id:"33-context-aware-prompting",children:"3.3 Context-Aware Prompting"}),"\n",(0,o.jsx)(e.p,{children:"Include environmental context in prompts:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Current robot location"}),"\n",(0,o.jsx)(e.li,{children:"Available objects and their positions"}),"\n",(0,o.jsx)(e.li,{children:"Recent actions and results"}),"\n",(0,o.jsx)(e.li,{children:"User preferences and history"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"4-action-planning-and-execution",children:"4. Action Planning and Execution"}),"\n",(0,o.jsx)(e.h3,{id:"41-action-representation",children:"4.1 Action Representation"}),"\n",(0,o.jsx)(e.p,{children:"Standardize action representations:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action Name"}),": String identifier for the action"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Parameters"}),": Dictionary of required parameters"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Preconditions"}),": Conditions that must be met"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Effects"}),": Expected outcomes"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Duration"}),": Estimated execution time"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"42-plan-validation",children:"4.2 Plan Validation"}),"\n",(0,o.jsx)(e.p,{children:"Validate plans before execution:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Check action availability"}),"\n",(0,o.jsx)(e.li,{children:"Verify parameter validity"}),"\n",(0,o.jsx)(e.li,{children:"Ensure preconditions are met"}),"\n",(0,o.jsx)(e.li,{children:"Detect potential conflicts"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"43-ros-2-action-integration",children:"4.3 ROS 2 Action Integration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionClient\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose\nfrom std_msgs.msg import String\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\n\nclass CognitivePlannerNode(Node):\n    def __init__(self):\n        super().__init__('cognitive_planner_node')\n\n        # Initialize LLM planner\n        self.llm_planner = LLMRobotPlanner(api_key=\"your-api-key\")\n\n        # ROS 2 publishers and action clients\n        self.move_client = ActionClient(self, MoveBaseAction, 'move_base')\n        self.command_pub = self.create_publisher(String, 'robot_commands', 10)\n\n        # Subscribe to natural language commands\n        self.command_sub = self.create_subscription(\n            String, 'natural_language_commands', self.command_callback, 10)\n\n    def command_callback(self, msg):\n        # Plan task using LLM\n        plan_result = self.llm_planner.plan_task(msg.data)\n\n        # Execute the plan\n        self.execute_plan(plan_result)\n\n    def execute_plan(self, plan_result):\n        action_sequence = plan_result.get('action_sequence', [])\n\n        for action in action_sequence:\n            action_name = action['action']\n            parameters = action['parameters']\n\n            if action_name == 'navigate_to':\n                self.execute_navigate_to(parameters['location'])\n            elif action_name == 'pick_object':\n                self.execute_pick_object(\n                    parameters['object_name'],\n                    parameters['location']\n                )\n            elif action_name == 'place_object':\n                self.execute_place_object(\n                    parameters['object_name'],\n                    parameters['location']\n                )\n            # Add other action handlers as needed\n\n    def execute_navigate_to(self, location):\n        # Convert location to coordinates and navigate\n        goal = MoveBaseGoal()\n        # Set goal coordinates based on location name\n        self.move_client.send_goal(goal)\n\n    def execute_pick_object(self, object_name, location):\n        # Implementation for picking object\n        pass\n\n    def execute_place_object(self, object_name, location):\n        # Implementation for placing object\n        pass\n"})}),"\n",(0,o.jsx)(e.h2,{id:"5-handling-ambiguity-and-errors",children:"5. Handling Ambiguity and Errors"}),"\n",(0,o.jsx)(e.h3,{id:"51-ambiguity-detection",children:"5.1 Ambiguity Detection"}),"\n",(0,o.jsx)(e.p,{children:"Identify when LLM responses are ambiguous:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Missing parameters"}),"\n",(0,o.jsx)(e.li,{children:"Unclear locations"}),"\n",(0,o.jsx)(e.li,{children:"Conflicting actions"}),"\n",(0,o.jsx)(e.li,{children:"Unavailable actions"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"52-clarification-strategies",children:"5.2 Clarification Strategies"}),"\n",(0,o.jsx)(e.p,{children:"Implement clarification mechanisms:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Ask for missing information"}),"\n",(0,o.jsx)(e.li,{children:"Present options for ambiguous choices"}),"\n",(0,o.jsx)(e.li,{children:"Confirm interpretations before execution"}),"\n",(0,o.jsx)(e.li,{children:"Use context to resolve ambiguity"}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class AmbiguityResolver:\n    def __init__(self):\n        self.known_locations = {\n            "kitchen": {"x": 1.0, "y": 2.0},\n            "living room": {"x": 3.0, "y": 1.0},\n            "bedroom": {"x": 0.5, "y": 4.0}\n        }\n\n    def resolve_ambiguity(self, plan_result, environment_context):\n        action_sequence = plan_result.get(\'action_sequence\', [])\n        resolved_actions = []\n\n        for action in action_sequence:\n            if self._has_ambiguity(action):\n                resolved_action = self._clarify_action(action, environment_context)\n                resolved_actions.append(resolved_action)\n            else:\n                resolved_actions.append(action)\n\n        plan_result[\'action_sequence\'] = resolved_actions\n        return plan_result\n\n    def _has_ambiguity(self, action):\n        # Check for missing or unclear parameters\n        if action[\'action\'] == \'navigate_to\':\n            location = action[\'parameters\'].get(\'location\', \'\').lower()\n            if location not in self.known_locations:\n                return True\n        return False\n\n    def _clarify_action(self, action, context):\n        # Implement clarification logic\n        # This might involve asking user for clarification\n        return action  # Placeholder\n'})}),"\n",(0,o.jsx)(e.h3,{id:"53-error-recovery",children:"5.3 Error Recovery"}),"\n",(0,o.jsx)(e.p,{children:"Implement error handling and recovery:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Monitor execution for failures"}),"\n",(0,o.jsx)(e.li,{children:"Retry failed actions"}),"\n",(0,o.jsx)(e.li,{children:"Generate alternative plans"}),"\n",(0,o.jsx)(e.li,{children:"Report errors to users"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"6-safety-and-validation",children:"6. Safety and Validation"}),"\n",(0,o.jsx)(e.h3,{id:"61-safety-constraints",children:"6.1 Safety Constraints"}),"\n",(0,o.jsx)(e.p,{children:"Implement safety checks:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Physical safety limits"}),"\n",(0,o.jsx)(e.li,{children:"Environmental constraints"}),"\n",(0,o.jsx)(e.li,{children:"User safety requirements"}),"\n",(0,o.jsx)(e.li,{children:"Robot capability limits"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"62-plan-verification",children:"6.2 Plan Verification"}),"\n",(0,o.jsx)(e.p,{children:"Verify plans meet safety requirements:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Check for dangerous actions"}),"\n",(0,o.jsx)(e.li,{children:"Validate environmental feasibility"}),"\n",(0,o.jsx)(e.li,{children:"Ensure robot can execute planned actions"}),"\n",(0,o.jsx)(e.li,{children:"Confirm safety constraints are met"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"63-human-in-the-loop",children:"6.3 Human-in-the-Loop"}),"\n",(0,o.jsx)(e.p,{children:"Include human oversight:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Plan approval before execution"}),"\n",(0,o.jsx)(e.li,{children:"Real-time monitoring"}),"\n",(0,o.jsx)(e.li,{children:"Emergency stop capabilities"}),"\n",(0,o.jsx)(e.li,{children:"Manual override options"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"7-performance-optimization",children:"7. Performance Optimization"}),"\n",(0,o.jsx)(e.h3,{id:"71-caching-strategies",children:"7.1 Caching Strategies"}),"\n",(0,o.jsx)(e.p,{children:"Improve performance with caching:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Cache common command interpretations"}),"\n",(0,o.jsx)(e.li,{children:"Store frequently used plans"}),"\n",(0,o.jsx)(e.li,{children:"Cache environmental information"}),"\n",(0,o.jsx)(e.li,{children:"Cache LLM responses when appropriate"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"72-local-processing",children:"7.2 Local Processing"}),"\n",(0,o.jsx)(e.p,{children:"Consider local processing options:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Run smaller models locally"}),"\n",(0,o.jsx)(e.li,{children:"Cache models in memory"}),"\n",(0,o.jsx)(e.li,{children:"Optimize API usage"}),"\n",(0,o.jsx)(e.li,{children:"Use edge computing for real-time tasks"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"73-asynchronous-processing",children:"7.3 Asynchronous Processing"}),"\n",(0,o.jsx)(e.p,{children:"Implement asynchronous processing:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Process commands in background"}),"\n",(0,o.jsx)(e.li,{children:"Execute actions while planning next steps"}),"\n",(0,o.jsx)(e.li,{children:"Handle multiple commands concurrently"}),"\n",(0,o.jsx)(e.li,{children:"Provide feedback during execution"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"8-advanced-cognitive-planning",children:"8. Advanced Cognitive Planning"}),"\n",(0,o.jsx)(e.h3,{id:"81-multi-modal-integration",children:"8.1 Multi-Modal Integration"}),"\n",(0,o.jsx)(e.p,{children:"Combine with other sensors:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Vision systems for object recognition"}),"\n",(0,o.jsx)(e.li,{children:"Audio systems for voice commands"}),"\n",(0,o.jsx)(e.li,{children:"Tactile sensors for manipulation"}),"\n",(0,o.jsx)(e.li,{children:"Environmental sensors for context"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"82-learning-and-adaptation",children:"8.2 Learning and Adaptation"}),"\n",(0,o.jsx)(e.p,{children:"Implement learning capabilities:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Learn from successful executions"}),"\n",(0,o.jsx)(e.li,{children:"Adapt to user preferences"}),"\n",(0,o.jsx)(e.li,{children:"Improve command understanding"}),"\n",(0,o.jsx)(e.li,{children:"Optimize plan efficiency"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"83-collaborative-planning",children:"8.3 Collaborative Planning"}),"\n",(0,o.jsx)(e.p,{children:"Enable multi-robot coordination:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Coordinate actions between multiple robots"}),"\n",(0,o.jsx)(e.li,{children:"Share environmental information"}),"\n",(0,o.jsx)(e.li,{children:"Handle complex multi-robot tasks"}),"\n",(0,o.jsx)(e.li,{children:"Manage resource allocation"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"9-practical-implementation",children:"9. Practical Implementation"}),"\n",(0,o.jsx)(e.h3,{id:"91-complete-cognitive-planning-system",children:"9.1 Complete Cognitive Planning System"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\nimport openai\nimport json\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom typing import Dict, Any, List\n\nclass CompleteCognitivePlanner(Node):\n    def __init__(self):\n        super().__init__(\'complete_cognitive_planner\')\n\n        # Initialize components\n        self.llm_client = openai.OpenAI(api_key="your-api-key")\n        self.ambiguity_resolver = AmbiguityResolver()\n\n        # ROS 2 interfaces\n        self.command_sub = self.create_subscription(\n            String, \'natural_language_commands\', self.process_command, 10)\n        self.action_pub = self.create_publisher(String, \'robot_actions\', 10)\n\n        # Context tracking\n        self.robot_location = "base_station"\n        self.environment_objects = {}\n\n        self.get_logger().info("Cognitive Planner initialized")\n\n    def process_command(self, msg):\n        try:\n            # Get environmental context\n            context = self._get_environment_context()\n\n            # Generate plan with LLM\n            plan = self._generate_plan(msg.data, context)\n\n            # Resolve ambiguities\n            resolved_plan = self.ambiguity_resolver.resolve_ambiguity(plan, context)\n\n            # Validate plan safety\n            if self._validate_plan_safety(resolved_plan):\n                # Execute plan\n                self._execute_plan(resolved_plan)\n            else:\n                self.get_logger().error("Plan failed safety validation")\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing command: {e}")\n\n    def _get_environment_context(self) -> Dict[str, Any]:\n        return {\n            "robot_location": self.robot_location,\n            "available_objects": self.environment_objects,\n            "time_of_day": "day",  # Could come from system\n            "user_preferences": {}  # Could be loaded from user profile\n        }\n\n    def _generate_plan(self, command: str, context: Dict[str, Any]) -> Dict[str, Any]:\n        system_prompt = f"""\n        You are a cognitive robot planner. Plan robot actions based on user commands.\n        Current context: {json.dumps(context)}\n\n        Available actions: navigate_to, pick_object, place_object, detect_object, wait, report_status\n        Respond in JSON format with action_sequence and reasoning.\n        """\n\n        response = self.llm_client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": command}\n            ],\n            temperature=0.1\n        )\n\n        return json.loads(response.choices[0].message.content)\n\n    def _validate_plan_safety(self, plan: Dict[str, Any]) -> bool:\n        # Implement safety validation logic\n        return True  # Placeholder\n\n    def _execute_plan(self, plan: Dict[str, Any]):\n        action_sequence = plan.get(\'action_sequence\', [])\n        for action in action_sequence:\n            self.execute_single_action(action)\n\n    def execute_single_action(self, action: Dict[str, Any]):\n        # Publish action to appropriate ROS 2 interface\n        action_msg = String()\n        action_msg.data = json.dumps(action)\n        self.action_pub.publish(action_msg)\n'})}),"\n",(0,o.jsx)(e.h2,{id:"10-testing-and-evaluation",children:"10. Testing and Evaluation"}),"\n",(0,o.jsx)(e.h3,{id:"101-plan-quality-metrics",children:"10.1 Plan Quality Metrics"}),"\n",(0,o.jsx)(e.p,{children:"Evaluate cognitive planning systems:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Success Rate"}),": Percentage of successfully executed commands"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Planning Time"}),": Time to generate action plans"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Accuracy"}),": Correctness of action interpretation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robustness"}),": Handling of ambiguous or complex commands"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"102-human-robot-interaction-metrics",children:"10.2 Human-Robot Interaction Metrics"}),"\n",(0,o.jsx)(e.p,{children:"Measure interaction quality:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"User Satisfaction"}),": Subjective evaluation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Task Completion"}),": Successful task execution"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Error Recovery"}),": Handling of failures"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Naturalness"}),": How natural the interaction feels"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Basic Integration"}),": Implement a simple LLM-based command interpreter"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Prompt Engineering"}),": Design effective prompts for specific robot tasks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Plan Validation"}),": Add safety checks to your cognitive planning system"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multi-step Tasks"}),": Create complex task planning with error handling"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"This week we explored cognitive planning using LLMs to translate natural language into ROS 2 actions. We learned about prompt engineering, action planning, ambiguity resolution, and safety considerations. Cognitive planning enables robots to understand complex, high-level commands and execute sophisticated behaviors that adapt to their environment and user needs."}),"\n",(0,o.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://platform.openai.com/docs/",children:"OpenAI API Documentation"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://navigation.ros.org/",children:"ROS 2 Navigation Documentation"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://arxiv.org/abs/2303.17015",children:"Large Language Models for Robotics"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://ieeexplore.ieee.org/document/9146621",children:"Cognitive Robotics Research"})}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>l});var t=i(6540);const o={},s=t.createContext(o);function a(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);