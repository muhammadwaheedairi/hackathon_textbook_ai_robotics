"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[46],{5177:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-3-ai-robot-brain/week-8-isaac-sim-reinforcement-learning/chapter-15-isaac-gym-gpu-rl","title":"Isaac Gym GPU-Accelerated RL","description":"Overview","source":"@site/docs/module-3-ai-robot-brain/week-8-isaac-sim-reinforcement-learning/chapter-15-isaac-gym-gpu-rl.md","sourceDirName":"module-3-ai-robot-brain/week-8-isaac-sim-reinforcement-learning","slug":"/module-3-ai-robot-brain/week-8-isaac-sim-reinforcement-learning/chapter-15-isaac-gym-gpu-rl","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/week-8-isaac-sim-reinforcement-learning/chapter-15-isaac-gym-gpu-rl","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadwaheedairi/hackathon_textbook_ai_robotics/edit/main/my-website/docs/module-3-ai-robot-brain/week-8-isaac-sim-reinforcement-learning/chapter-15-isaac-gym-gpu-rl.md","tags":[],"version":"current","sidebarPosition":15,"frontMatter":{"title":"Isaac Gym GPU-Accelerated RL","sidebar_label":"Chapter 15: Isaac Gym GPU RL","sidebar_position":15},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 14: Nav2 Integration","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/week-7-isaac-ros-hardware-accelerated/chapter-14-nav2-integration"},"next":{"title":"Chapter 16: Domain Randomization","permalink":"/hackathon_textbook_ai_robotics/docs/module-3-ai-robot-brain/week-8-isaac-sim-reinforcement-learning/chapter-16-domain-randomization"}}');var r=i(4848),t=i(8453);const s={title:"Isaac Gym GPU-Accelerated RL",sidebar_label:"Chapter 15: Isaac Gym GPU RL",sidebar_position:15},o="Chapter 15: Isaac Gym GPU-Accelerated RL",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Isaac Gym Fundamentals",id:"isaac-gym-fundamentals",level:2},{value:"Core Concepts",id:"core-concepts",level:3},{value:"GPU-Accelerated Simulation",id:"gpu-accelerated-simulation",level:3},{value:"Setting up RL Environments",id:"setting-up-rl-environments",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Environment Definition Structure",id:"environment-definition-structure",level:3},{value:"Robot Setup in RL Environment",id:"robot-setup-in-rl-environment",level:3},{value:"RL Training Script",id:"rl-training-script",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"What&#39;s Next",id:"whats-next",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-15-isaac-gym-gpu-accelerated-rl",children:"Chapter 15: Isaac Gym GPU-Accelerated RL"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This chapter explores GPU-accelerated reinforcement learning using Isaac Gym integrated with Isaac Sim. You'll learn how to train robotic policies using parallel environments and hardware acceleration for efficient learning."}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.admonition,{title:"Learning Objectives",type:"info",children:[(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand Isaac Gym's GPU-accelerated RL architecture"}),"\n",(0,r.jsx)(n.li,{children:"Set up parallel RL environments in Isaac Sim"}),"\n",(0,r.jsx)(n.li,{children:"Implement RL training pipelines for robotic tasks"}),"\n",(0,r.jsx)(n.li,{children:"Train policies using PPO and other RL algorithms"}),"\n"]})]}),"\n",(0,r.jsx)(n.h2,{id:"isaac-gym-fundamentals",children:"Isaac Gym Fundamentals"}),"\n",(0,r.jsx)(n.p,{children:"Isaac Gym leverages GPU parallelism for reinforcement learning, enabling thousands of parallel environments on a single GPU, physics simulation computed in parallel, sensor data generated simultaneously, and actions applied across all environments at once."}),"\n",(0,r.jsx)(n.h3,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Environment"}),": The world where the agent acts"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Agent"}),": The learning entity that interacts with the environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Observation"}),": Sensor data from the environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action"}),": Commands sent to the robot"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reward"}),": Feedback signal for learning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Episode"}),": Complete sequence from start to termination"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"gpu-accelerated-simulation",children:"GPU-Accelerated Simulation"}),"\n",(0,r.jsx)(n.p,{children:"Isaac Gym leverages GPU parallelism where each environment runs in parallel on GPU threads, physics simulation computed in parallel, sensor data generated simultaneously, and actions applied across all environments at once."}),"\n",(0,r.jsx)(n.h2,{id:"setting-up-rl-environments",children:"Setting up RL Environments"}),"\n",(0,r.jsx)(n.p,{children:"Key parameters for RL environments include number of parallel environments (balance between speed and memory), episode length (maximum steps before reset), action and observation spaces (define the problem structure), and reward shaping (design rewards that guide learning)."}),"\n",(0,r.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,r.jsx)(n.h3,{id:"environment-definition-structure",children:"Environment Definition Structure"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.articulations import ArticulationView\nfrom omni.isaac.core.utils.torch.maths import *\nfrom omni.isaac.core.objects import DynamicCuboid\nfrom omni.isaac.core.prims import RigidPrimView\nfrom omni.isaac.core.tasks import BaseTask\nfrom omni.isaac.core.utils.prims import create_prim\nimport numpy as np\n\nclass IsaacSimRLTask(BaseTask):\n    def __init__(self, name, offset=None):\n        super().__init__(name=name, offset=offset)\n        self._num_envs = 100\n        self._env_spacing = 2.0\n        self._action_space = 7\n        self._observation_space = 28\n\n    def set_up_scene(self, scene):\n        world = self.get_world()\n        world.scene.add_default_ground_plane()\n        return\n\n    def get_observations(self):\n        return self._observations\n\n    def get_extras(self):\n        return {}\n\n    def pre_physics_step(self, actions):\n        pass\n\n    def post_reset(self):\n        pass\n"})}),"\n",(0,r.jsx)(n.h3,{id:"robot-setup-in-rl-environment",children:"Robot Setup in RL Environment"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def setup_robot_environment():\n    add_reference_to_stage(\n        usd_path="/Isaac/Robots/Franka/franka_instanceable.usd",\n        prim_path="/World/envs/env_0/robot"\n    )\n\n    robot = ArticulationView(\n        prim_path="/World/envs/.*/robot",\n        name="robot_view",\n        reset_xform_properties=False,\n    )\n\n    cube = DynamicCuboid(\n        prim_path="/World/envs/env_0/cube",\n        name="cube",\n        position=np.array([0.5, 0.0, 0.1]),\n        size=0.1,\n        color=np.array([0.9, 0.1, 0.1])\n    )\n\n    return robot, cube\n'})}),"\n",(0,r.jsx)(n.h3,{id:"rl-training-script",children:"RL Training Script"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom omni.isaac.gym.vec_env import VecEnvBase\n\nclass RLTrainer:\n    def __init__(self, env, policy_network, learning_rate=3e-4):\n        self.env = env\n        self.policy = policy_network\n        self.optimizer = optim.Adam(policy_network.parameters(), lr=learning_rate)\n\n    def train_step(self):\n        observations, rewards, dones, info = self.env.step(actions)\n\n        loss = self.compute_loss(observations, rewards, dones)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        return loss\n\n    def compute_loss(self, observations, rewards, dones):\n        pass\n"})}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Isaac Gym provides GPU-accelerated reinforcement learning capabilities for training robotic policies efficiently. Parallel environment execution enables rapid policy learning, while integration with Isaac Sim provides high-fidelity simulation for robust policy development."}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsx)(n.admonition,{title:"Key Takeaways",type:"tip",children:(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"GPU acceleration enables thousands of parallel RL environments"}),"\n",(0,r.jsx)(n.li,{children:"Isaac Gym integrates seamlessly with Isaac Sim for high-fidelity training"}),"\n",(0,r.jsx)(n.li,{children:"Proper environment design and reward shaping are critical for learning success"}),"\n",(0,r.jsx)(n.li,{children:"Parallel execution dramatically reduces training time for robotic policies"}),"\n"]})}),"\n",(0,r.jsx)(n.h2,{id:"whats-next",children:"What's Next"}),"\n",(0,r.jsx)(n.p,{children:"In the next chapter, we'll explore domain randomization techniques for improving sim-to-real transfer, enabling policies trained in simulation to work effectively on real robots."})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var a=i(6540);const r={},t=a.createContext(r);function s(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);