"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6627],{1145:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vision-language-action/week-10-cognitive-planning/chapter-19-llm-cognitive-planning","title":"LLM Cognitive Planning","description":"Overview","source":"@site/docs/module-4-vision-language-action/week-10-cognitive-planning/chapter-19-llm-cognitive-planning.md","sourceDirName":"module-4-vision-language-action/week-10-cognitive-planning","slug":"/module-4-vision-language-action/week-10-cognitive-planning/chapter-19-llm-cognitive-planning","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-10-cognitive-planning/chapter-19-llm-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadwaheedairi/hackathon_textbook_ai_robotics/edit/main/my-website/docs/module-4-vision-language-action/week-10-cognitive-planning/chapter-19-llm-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":19,"frontMatter":{"title":"LLM Cognitive Planning","sidebar_label":"Chapter 19: LLM Cognitive Planning","sidebar_position":19},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 18: Voice ROS 2 Integration","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-9-voice-to-action-with-openai-whisper/chapter-18-voice-ros2-integration"},"next":{"title":"Chapter 20: Action Planning & Safety","permalink":"/hackathon_textbook_ai_robotics/docs/module-4-vision-language-action/week-10-cognitive-planning/chapter-20-action-planning-safety"}}');var o=t(4848),a=t(8453);const r={title:"LLM Cognitive Planning",sidebar_label:"Chapter 19: LLM Cognitive Planning",sidebar_position:19},s="Chapter 19: LLM Cognitive Planning",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Cognitive Robotics with LLMs",id:"introduction-to-cognitive-robotics-with-llms",level:2},{value:"Role of LLMs in Cognitive Robotics",id:"role-of-llms-in-cognitive-robotics",level:3},{value:"LLM Integration for Robotics",id:"llm-integration-for-robotics",level:2},{value:"Choosing the Right LLM",id:"choosing-the-right-llm",level:3},{value:"Popular LLM Options for Robotics",id:"popular-llm-options-for-robotics",level:3},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"API Integration Patterns",id:"api-integration-patterns",level:3},{value:"Few-Shot Learning Examples",id:"few-shot-learning-examples",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"What&#39;s Next",id:"whats-next",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-19-llm-cognitive-planning",children:"Chapter 19: LLM Cognitive Planning"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This chapter explores using Large Language Models (LLMs) for cognitive planning in robotics. You'll learn how to leverage LLMs for natural language understanding, task planning, and translating high-level commands into executable robot actions."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.admonition,{title:"Learning Objectives",type:"info",children:[(0,o.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate LLMs for robot task planning and reasoning"}),"\n",(0,o.jsx)(n.li,{children:"Design effective prompts for robotics applications"}),"\n",(0,o.jsx)(n.li,{children:"Implement LLM-based command interpretation systems"}),"\n",(0,o.jsx)(n.li,{children:"Translate natural language to structured robot actions"}),"\n"]})]}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-cognitive-robotics-with-llms",children:"Introduction to Cognitive Robotics with LLMs"}),"\n",(0,o.jsx)(n.p,{children:"Cognitive robotics involves creating robots that can understand high-level, natural language commands, reason about the environment and task requirements, plan complex sequences of actions, adapt to unexpected situations, and learn from experience and interaction."}),"\n",(0,o.jsx)(n.h3,{id:"role-of-llms-in-cognitive-robotics",children:"Role of LLMs in Cognitive Robotics"}),"\n",(0,o.jsx)(n.p,{children:"Large Language Models enhance cognitive robotics by providing natural language understanding (interpreting human instructions), reasoning (planning multi-step actions), knowledge integration (accessing world knowledge), context awareness (understanding situational context), and adaptation (learning from interaction)."}),"\n",(0,o.jsx)(n.h2,{id:"llm-integration-for-robotics",children:"LLM Integration for Robotics"}),"\n",(0,o.jsx)(n.h3,{id:"choosing-the-right-llm",children:"Choosing the Right LLM"}),"\n",(0,o.jsx)(n.p,{children:"Consider these factors for robotics applications: response time (real-time vs. batch processing), accuracy (understanding complex commands), cost (API usage and computational requirements), privacy (handling sensitive data), and customization (fine-tuning capabilities)."}),"\n",(0,o.jsx)(n.h3,{id:"popular-llm-options-for-robotics",children:"Popular LLM Options for Robotics"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"OpenAI GPT"}),": High capability, good documentation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Anthropic Claude"}),": Strong reasoning, safety focus"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Google Gemini"}),": Multimodal capabilities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Open Source Models"}),": Mistral, Llama (for local deployment)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Specialized Models"}),": Fine-tuned for robotics tasks"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,o.jsx)(n.p,{children:"Effective system prompts for robotics should include role definition (clearly define the LLM's role), action vocabulary (list available robot actions), format requirements (specify output format), context information (provide relevant environment info), and safety guidelines (include safety constraints)."}),"\n",(0,o.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,o.jsx)(n.h3,{id:"api-integration-patterns",children:"API Integration Patterns"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\nimport json\nfrom typing import Dict, List, Any\n\nclass LLMRobotPlanner:\n    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):\n        openai.api_key = api_key\n        self.model = model\n        self.system_prompt = self._build_system_prompt()\n\n    def _build_system_prompt(self) -> str:\n        return """\n        You are a robotic task planner. Your job is to interpret natural language commands\n        and translate them into structured robot actions for a ROS 2 system.\n\n        Available actions:\n        - move_to(location): Move robot to specified location\n        - pick_object(object_name, location): Pick up an object\n        - place_object(object_name, location): Place an object at location\n        - navigate_to(location): Navigate to location\n        - detect_object(object_type): Detect objects of specified type\n        - wait(duration): Wait for specified duration\n        - report_status(): Report current robot status\n\n        Respond with a JSON object containing:\n        {\n            "action_sequence": [\n                {\n                    "action": "action_name",\n                    "parameters": {"param1": "value1", ...}\n                }\n            ],\n            "reasoning": "Brief explanation of the plan"\n        }\n\n        Be specific about locations and objects. If information is ambiguous,\n        ask for clarification.\n        """\n\n    def plan_task(self, command: str) -> Dict[str, Any]:\n        response = openai.ChatCompletion.create(\n            model=self.model,\n            messages=[\n                {"role": "system", "content": self.system_prompt},\n                {"role": "user", "content": command}\n            ],\n            temperature=0.1\n        )\n\n        try:\n            result = json.loads(response.choices[0].message[\'content\'])\n            return result\n        except json.JSONDecodeError:\n            return {"action_sequence": [], "reasoning": "Failed to parse response"}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"few-shot-learning-examples",children:"Few-Shot Learning Examples"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def get_few_shot_examples() -> List[Dict[str, str]]:\n    return [\n        {\n            "role": "user",\n            "content": "Go to the kitchen and bring me a cup of coffee."\n        },\n        {\n            "role": "assistant",\n            "content": json.dumps({\n                "action_sequence": [\n                    {"action": "navigate_to", "parameters": {"location": "kitchen"}},\n                    {"action": "detect_object", "parameters": {"object_type": "cup"}},\n                    {"action": "pick_object", "parameters": {"object_name": "cup", "location": "kitchen counter"}},\n                    {"action": "navigate_to", "parameters": {"location": "coffee machine"}},\n                    {"action": "place_object", "parameters": {"object_name": "cup", "location": "coffee machine tray"}},\n                    {"action": "navigate_to", "parameters": {"location": "your location"}}\n                ],\n                "reasoning": "First navigate to kitchen, detect cup, pick it up, then go to coffee machine to place cup, then return."\n            })\n        }\n    ]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"LLMs provide powerful cognitive capabilities for robotics, enabling natural language understanding and complex task planning. Proper prompt engineering and API integration allow robots to interpret high-level commands and generate executable action sequences."}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsx)(n.admonition,{title:"Key Takeaways",type:"tip",children:(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"LLMs enable natural language understanding for robot commands"}),"\n",(0,o.jsx)(n.li,{children:"Effective prompt engineering is critical for reliable robot planning"}),"\n",(0,o.jsx)(n.li,{children:"Few-shot learning improves command interpretation accuracy"}),"\n",(0,o.jsx)(n.li,{children:"Structured output formats enable seamless ROS 2 integration"}),"\n"]})}),"\n",(0,o.jsx)(n.h2,{id:"whats-next",children:"What's Next"}),"\n",(0,o.jsx)(n.p,{children:"In the next chapter, we'll explore action planning validation and safety considerations, learning how to ensure LLM-generated plans are safe and executable before deployment."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const o={},a=i.createContext(o);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);